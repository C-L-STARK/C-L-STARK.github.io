
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="FastDoc is a website that you can get started with FastX AI in minutes.">
      
      
      
        <link rel="canonical" href="https://doc.fastx-ai.com/ml/1_58_llm_extreme_quantization/">
      
      
        <link rel="prev" href="../../big-data/">
      
      
        <link rel="next" href="../2023-in-llms/">
      
      
        <link rel="alternate" type="application/rss+xml" title="RSS feed" href="../../feed_rss_created.xml">
        <link rel="alternate" type="application/rss+xml" title="RSS feed of updated content" href="../../feed_rss_updated.xml">
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.46">
    
    
      
        <title>Fine-tuning LLMs to 1.58bit: extreme quantization made easy - FastDocs</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.6f8fc17f.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  

<script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-XXXXXXXXXX"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-XXXXXXXXXX",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-XXXXXXXXXX",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script>
  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
      
        <meta  property="og:type"  content="website" >
      
        <meta  property="og:title"  content="Fine-tuning LLMs to 1.58bit: extreme quantization made easy - FastDocs" >
      
        <meta  property="og:description"  content="FastDoc is a website that you can get started with FastX AI in minutes." >
      
        <meta  property="og:image"  content="https://doc.fastx-ai.com/assets/images/social/ml/1_58_llm_extreme_quantization.png" >
      
        <meta  property="og:image:type"  content="image/png" >
      
        <meta  property="og:image:width"  content="1200" >
      
        <meta  property="og:image:height"  content="630" >
      
        <meta  property="og:url"  content="https://doc.fastx-ai.com/ml/1_58_llm_extreme_quantization/" >
      
        <meta  name="twitter:card"  content="summary_large_image" >
      
        <meta  name="twitter:title"  content="Fine-tuning LLMs to 1.58bit: extreme quantization made easy - FastDocs" >
      
        <meta  name="twitter:description"  content="FastDoc is a website that you can get started with FastX AI in minutes." >
      
        <meta  name="twitter:image"  content="https://doc.fastx-ai.com/assets/images/social/ml/1_58_llm_extreme_quantization.png" >
      
    
    
   <link href="../../assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }
    .gscrollbar-fixer { padding-right: 15px; }
    .gdesc-inner { font-size: 0.75rem; }
    body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
    body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
    body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}</style> <script src="../../assets/javascripts/glightbox.min.js"></script></head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#llms-158" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
        <aside class="md-banner">
          <div class="md-banner__inner md-grid md-typeset">
            
              <button class="md-banner__button md-icon" aria-label="Don't show this again">
                
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
              </button>
            
            
<p style="text-align: center">
  Welcome to <span style="font-size: bold">FastDocs</span>! Just feel free to start read docs!
</p>

          </div>
          
            <script>var el=document.querySelector("[data-md-component=announce]");if(el){var content=el.querySelector(".md-typeset");__md_hash(content.innerHTML)===__md_get("__announce")&&(el.hidden=!0)}</script>
          
        </aside>
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="FastDocs" class="md-header__button md-logo" aria-label="FastDocs" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            FastDocs
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Fine-tuning LLMs to 1.58bit: extreme quantization made easy
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9zM20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12zm-9.15 3.96h2.3L12 9z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to system preference"  type="radio" name="__palette" id="__palette_2">
    
      <label class="md-header__button md-icon" title="Switch to system preference" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256l137.3-137.4c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/C-L-STARK/C-L-STARK.github.io" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><!--! Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1M480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2m-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3m-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1"/></svg>
  </div>
  <div class="md-source__repository">
    C-L-STARK/C-L-STARK.github.io
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
    
  
  主页

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../backend/" class="md-tabs__link">
        
  
    
  
  后端

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../web/" class="md-tabs__link">
        
  
    
  
  前端

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../client/" class="md-tabs__link">
        
  
    
  
  客户端

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../pc/" class="md-tabs__link">
        
  
    
  
  桌面端

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../big-data/" class="md-tabs__link">
        
  
    
  
  大数据

      </a>
    </li>
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="./" class="md-tabs__link">
          
  
    
  
  人工智能

        </a>
      </li>
    
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../ops/" class="md-tabs__link">
        
  
    
  
  运维

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../blog/" class="md-tabs__link">
          
  
    
  
  博客

        </a>
      </li>
    
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../resume/" class="md-tabs__link">
        
  
    
  
  简历模板

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="FastDocs" class="md-nav__button md-logo" aria-label="FastDocs" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    FastDocs
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/C-L-STARK/C-L-STARK.github.io" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><!--! Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1M480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2m-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3m-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1"/></svg>
  </div>
  <div class="md-source__repository">
    C-L-STARK/C-L-STARK.github.io
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    主页
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../backend/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    后端
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../web/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    前端
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../client/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    客户端
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pc/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    桌面端
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../big-data/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    大数据
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
        
        
      
      
    
    
      
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" checked>
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="">
            
  
  <span class="md-ellipsis">
    人工智能
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            人工智能
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Fine-tuning LLMs to 1.58bit: extreme quantization made easy
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Fine-tuning LLMs to 1.58bit: extreme quantization made easy
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    <span class="md-ellipsis">
      目录
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      简介
    </span>
  </a>
  
    <nav class="md-nav" aria-label="简介">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformers" class="md-nav__link">
    <span class="md-ellipsis">
      如何在 Transformers 中使用
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bitnet" class="md-nav__link">
    <span class="md-ellipsis">
      更深入地了解什么是BitNet
    </span>
  </a>
  
    <nav class="md-nav" aria-label="更深入地了解什么是BitNet">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    <span class="md-ellipsis">
      训练
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    <span class="md-ellipsis">
      推理
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#158" class="md-nav__link">
    <span class="md-ellipsis">
      1.58比特的预训练结果
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#158_1" class="md-nav__link">
    <span class="md-ellipsis">
      1.58比特的微调
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1.58比特的微调">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#100btoken" class="md-nav__link">
    <span class="md-ellipsis">
      扩展到100B个token!
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    <span class="md-ellipsis">
      在更小的模型上的实验
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    <span class="md-ellipsis">
      对比与结论
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    <span class="md-ellipsis">
      使用的算子和测试标准
    </span>
  </a>
  
    <nav class="md-nav" aria-label="使用的算子和测试标准">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gpu" class="md-nav__link">
    <span class="md-ellipsis">
      基础的GPU概念: 线程, 块, 和共享内存
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    <span class="md-ellipsis">
      矩阵乘法中的挑战
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_9" class="md-nav__link">
    <span class="md-ellipsis">
      分块的概念
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#triton" class="md-nav__link">
    <span class="md-ellipsis">
      Triton算子
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_10" class="md-nav__link">
    <span class="md-ellipsis">
      代码解析
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_11" class="md-nav__link">
    <span class="md-ellipsis">
      基准测试
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_12" class="md-nav__link">
    <span class="md-ellipsis">
      结论
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_13" class="md-nav__link">
    <span class="md-ellipsis">
      致谢
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_14" class="md-nav__link">
    <span class="md-ellipsis">
      更多资源
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../2023-in-llms/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023, 开源大模型之年
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../3d-assets/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    手把手教你使用人工智能生成 3D 素材
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../4bit-transformers-bitsandbytes/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    用 bitsandbytes、4 比特量化和 QLoRA 打造亲民的 LLM
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Llama2-for-non-engineers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    非工程师指南：训练 LLaMA 2 聊天机器人
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Lora-for-sequence-classification-with-Roberta-Llama-Mistral/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    在灾难推文分析场景上比较用 LoRA 微调 Roberta、Llama 2 和 Mistral 的过程及表现
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../_policy-ntia-rfc/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    人工智能政策@🤗：回应美国国家电信和信息管理局（ NTIA ）关于人工智能问责制的评论请求
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../accelerate-v1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Accelerate 1.0.0
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../accelerated-inference/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    如何成功将 🤗 API 客户的 transformer 模型推理速度加快 100 倍
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../agents/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    授权调用：介绍 Transformers 智能体 2.0  
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../aivsai/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    AI 大战 AI，一个深度强化学习多智能体竞赛系统
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../arena-tts/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TTS 擂台: 文本转语音模型的自由搏击场
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../asr-diarization/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用 Hugging Face 推理终端搭建强大的“语音识别 + 说话人分割 + 投机解码”工作流
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../assisted-generation-support-gaudi/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    英特尔 Gaudi 加速辅助生成
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../assisted-generation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    辅助生成：低延迟文本生成的新方向
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../audioldm2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    AudioLDM 2，加速⚡️！
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../autoformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Transformer 模型能够有效地进行时间序列预测 (使用 Autoformer)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../beating-gaia/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Transformers 代码智能体成功刷榜 GAIA
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../big-bird/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    深入理解 BigBird 的块稀疏注意力
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../blip-2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用 BLIP-2 零样本“图生文”
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bloom-inference-optimization/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    优化故事: BLOOM 模型推理
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bloom-inference-pytorch-scripts/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用 DeepSpeed 和 Accelerate 进行超快 BLOOM 模型推理
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bloom-megatron-deepspeed/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    千亿参数开源大模型 BLOOM 背后的技术
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bridgetower/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用 Habana Gaudi2 加速视觉语言模型 BridgeTower
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../chat-templates/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    聊天模板：无声性能杀手的终结
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../chinese-ai-expansion/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    中国 AI 出海现状概述
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../chinese-language-blog/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Hugging Face 中文博客正式发布！
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cloudflare-workers-ai/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    为 Hugging Face 用户带来无服务器 GPU 推理服务
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../codellama/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Code Llama：Llama 2 学会写代码了！
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../community-datasets/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    数据好合：Argilla 和 Hugging Face Spaces 赋能社区合力构建更好的数据集
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../constrained-beam-search/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    在 🤗 Transformers 中使用约束波束搜索引导文本生成
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../controlnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用 🧨 Diffusers 实现 ControlNet 高速推理
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cosmopedia/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Cosmopedia：如何为大语言模型预训练构建大规模合成数据集
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cost-efficient-rag-applications-with-intel/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    利用英特尔 Gaudi 2 和至强 CPU 构建经济高效的企业级 RAG 应用
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cv_state/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Hugging Face 中计算机视觉的现状
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../daily-papers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Hugging Face 论文平台 Daily Papers 功能全解析
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../dedup/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    BigCode 背后的大规模数据去重
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../deep-learning-with-proteins/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    蛋白质深度学习
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../deepspeed-to-fsdp-and-back/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    从 DeepSpeed 到 FSDP，再回到 Hugging Face Accelerate
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../deploy-deepfloydif-using-bentoml/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用 BentoML 部署 🤗 Hugging Face 上的模型：DeepFloyd IF 实战
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../deploy-with-openvino/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用 Optimum-Intel 和 OpenVINO GenAI 优化和部署模型
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../dialog-agents/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    是什么让对话代理有用？
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../diffusers-turns-1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    🤗 Diffusers 一岁啦 !
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../docmatix/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Docmatix - 超大文档视觉问答数据集
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../document-ai/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    加速 Document AI (文档智能) 发展
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../dpo-trl/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用 DPO 微调 Llama 2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../dpo_vlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    为视觉语言多模态模型进行偏好优化
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../dreambooth/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用 Diffusers 通过 Dreambooth 技术来训练 Stable Diffusion
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../dynamic_speculation_lookahead/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    更快的辅助生成: 动态推测
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../elixir-bumblebee/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    从 GPT2 到 Stable Diffusion：Elixir 社区迎来了 Hugging Face
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../embedding-quantization/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    用于显著提高检索速度和降低成本的二进制和标量嵌入量化
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../encoder-decoder/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    基于 Transformers 的编码器-解码器模型
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../encrypted-llm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用 FHE 实现加密大语言模型
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ethics-diffusers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    开发 Diffusers 库的道德行为指南
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ethics-soc-3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    道德与社会问题简报 #3: Hugging Face 上的道德开放性
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ethics-soc-4/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Ethics and Society Newsletter #4: Bias in Text-to-Image Models
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../falcon-180b/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Falcon 180B 登陆 Hugging Face Hub 🔥
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../falcon/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Falcon 登陆 Hugging Face 生态
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../falconmamba/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Falcon Mamba: 首个高效的无注意力机制 7B 模型
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../fine-tune-whisper/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用 🤗 Transformers 为多语种语音识别任务微调 Whisper 模型
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../fine-video/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    揭秘 FineVideo 数据集构建的背后的秘密
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../finetune-florence2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    微调 Florence-2 - 微软的尖端视觉语言模型
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../game-jam-first-edition-results/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    首届开源 AI 游戏挑战赛事结果
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gaussian-splatting/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    3D 高斯点染简介
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gemma-july-update/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Google 最新发布： Gemma 2 2B, ShieldGemma 和 Gemma Scope
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gemma-peft/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用 Hugging Face 微调 Gemma 模型
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gemma/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    欢迎 Gemma: Google 最新推出开放大语言模型
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gemma2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Google 发布最新开放大语言模型 Gemma 2，现已登陆 Hugging Face Hub
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generative-ai-models-on-intel-cpu/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    越小越好：Q8-Chat，在英特尔至强 CPU 上体验高效的生成式 AI
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../getting-started-habana/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    基于 Habana Gaudi 的 Transformers 入门
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../google-cloud-model-garden/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    在 Google Cloud 上轻松部署开放大语言模型
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gptq-integration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用 AutoGPTQ 和 transformers 让大语言模型更轻量化
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gradio-5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Gradio 5 现已发布
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gradio-lite/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Gradio-Lite: 完全在浏览器里运行的无服务器 Gradio
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gradio-reload/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用 Gradio 的“热重载”模式快速开发 AI 应用
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../graphml-classification/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用 Transformers 进行图分类
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../habana-gaudi-2-benchmark/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    更快的训练和推理：对比 Habana Gaudi®2 和英伟达 A100 80GB
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../habana-gaudi-2-bloom/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    大语言模型快速推理：在 Habana Gaudi2 上推理 BLOOMZ
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../hf-bitsandbytes-integration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    大规模 Transformer 模型 8 比特矩阵乘简介 - 基于 Hugging Face Transformers、Accelerate 以及 bitsandbytes
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../how-to-generate/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    如何生成文本：通过 Transformers 用不同的解码方法生成文本
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../hugging-face-wiz-security-blog/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Hugging Face 与 Wiz Research 合作提高人工智能安全性
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../huggy-lingo/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Huggy Lingo：利用机器学习改进 Hugging Face Hub 上的语言元数据
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../idefics/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    IDEFICS 简介：最先进视觉语言模型的开源复现
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../idefics2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Idefics2 简介：为社区而生的强大 8B 视觉语言模型
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../if/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    在免费版 Google Colab 上使用 🧨 diffusers 运行 IF
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../image-similarity/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    基于 Hugging Face Datasets 和 Transformers 的图像相似性搜索
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../inference-endpoints-llm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    用 Hugging Face 推理端点部署 LLM
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../inference-update/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Hugging Face 提供的推理（Inference）解决方案
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../infini-attention/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    一次失败的实验——无限注意力，我们为什么坚持实验
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../informer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用 Informer 进行多元概率时间序列预测
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../instruction-tuning-sd/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用 InstructPix2Pix 对 Stable Diffusion 进行指令微调
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../intel-fast-embedding/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    利用 🤗 Optimum Intel 和 fastRAG 在 CPU 上优化文本嵌入
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../intel-protein-language-model-protst/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    在英特尔 Gaudi 2 上加速蛋白质语言模型 ProtST
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../intel-sapphire-rapids-inference/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    CPU 推理 | 使用英特尔 Sapphire Rapids 加速 PyTorch Transformers
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../intel-sapphire-rapids/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用英特尔 Sapphire Rapids 加速 PyTorch Transformers 模型（第一部分）
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../intel-starcoder-quantization/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用 🤗 Optimum Intel 在英特尔至强上加速 StarCoder：Q8/Q4 及投机解码
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../intro-graphml/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    一文带你入门图机器学习
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../introducing-csearch/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    在 Transformers 中使用对比搜索生成可媲美人类水平的文本🤗
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../introduction-to-ggml/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ggml 简介
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../jat/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    万事通，专精部分领域的多功能 Transformer 智能体
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../kv-cache-quantization/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    用 KV 缓存量化解锁长文本生成
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../langchain/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Hugging Face x LangChain：全新 LangChain 合作伙伴包
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../large-language-models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    大语言模型：新的摩尔定律？
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lcm_lora/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用 LCM LoRA 4 步完成 SDXL 推理
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../leaderboard-bigcodebench/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    BigCodeBench: 继 HumanEval 之后的新一代代码生成基准测试
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../leaderboard-decodingtrust/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    来自 AI Secure 实验室的 LLM 安全排行榜简介
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../leaderboard-medicalllm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    开源医疗大模型排行榜：健康领域大模型基准测试
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../leaderboard-patronus/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    企业场景排行榜简介：现实世界用例排行榜
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../llama2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Llama 2 来袭 - 在 Hugging Face 上玩转它
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../llama3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    欢迎 Llama 3：Meta 的新一代开源大语言模型
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../llama31/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Llama 3.1：405B/70B/8B 模型的多语言与长上下文能力解析
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../llama32/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    现在 Llama 具备视觉能力并可以在你的设备上运行 - 欢迎使用 Llama 3.2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../long-range-transformers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    长程 transformer 模型
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lora/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用 LoRA 进行 Stable Diffusion 的高效参数微调
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mask2former/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    通用图像分割任务：使用 Mask2Former 和 OneFormer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../matryoshka/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    🪆 俄罗斯套娃嵌入模型
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../megatron-training/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    如何使用 Megatron-LM 训练语言模型
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mixtral/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    欢迎 Mixtral - 当前 Hugging Face 上最先进的 MoE 模型
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ml-for-games-1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    基于AI进行游戏开发：5天！创建一个农场游戏！第1部分
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ml-for-games-2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用 ChatGPT 启发游戏创意｜基于 AI 5 天创建一个农场游戏，第 2 天
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ml-for-games-3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    AI 制作 3D 素材｜基于 AI 5 天创建一个农场游戏，第 3 天
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ml-for-games-4/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    制作 2D 素材｜基于 AI 5 天创建一个农场游戏，第 4 天
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ml-for-games-5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ChatGPT 设计游戏剧情 | 基于 AI 5 天创建一个农场游戏，完结篇！
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mms_adapters/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    微调用于多语言 ASR 的 MMS 适配器模型
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../moe/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    混合专家模型（MoE）详解
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../multi-lora-serving/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TGI 多-LoRA：部署一次，搞定 30 个模型的推理服务
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../noob_intro_transformers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Hugging Face Transformers 萌新完全指南
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../open-llm-leaderboard-drop/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    开放 LLM 排行榜：深入研究 DROP
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../open-llm-leaderboard-mmlu/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Open LLM 排行榜近况
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../open-llm-leaderboard-rlhf/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    基础大模型能像人类一样标注数据吗？
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../open-source-llms-as-agents/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    开源大语言模型作为 LangChain 智能体
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../optimize-llm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    面向生产的 LLM 优化
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../optimizing-bark/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用 🤗 Transformers 优化 Bark
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../optimum-onnxruntime-training/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Optimum + ONNX Runtime: 更容易、更快地训练你的 Hugging Face 模型
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../os-llms/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Hugging Face 的文本生成和大语言模型的开源生态
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../overview-quantization-transformers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    🤗 Transformers 中原生支持的量化方案概述
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../packing-with-FA2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    通过打包 Flash Attention 来提升 Hugging Face 训练效率
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../paligemma/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PaliGemma 正式发布 — Google 最新发布的前沿开放视觉语言模型
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../password-git-deprecation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Hub 上的 Git 操作不再支持使用密码验证
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../peft/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    🤗 PEFT：在低资源硬件上对十亿规模模型进行参数高效微调
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../personal-copilot/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    个人编程助手：训练你自己的编码助手
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../phi2-intel-meteor-lake/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    笔记本电脑上的聊天机器人：在英特尔 Meteor Lake 上运行 Phi-2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../presidio-pii-detection/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    在 Hub 上使用 Presidio 进行自动 PII 检测实验
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../putting_rl_back_in_rlhf_with_rloo/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    将强化学习重新引入 RLHF
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pycharm-integration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Hugging Face 与 PyCharm 深度集成：轻松引入丰富的 AI 模型
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pytorch-ddp-accelerate-transformers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    从 PyTorch DDP 到 Accelerate 到 Trainer，轻松掌握分布式训练
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pytorch-fsdp/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用 PyTorch 完全分片数据并行技术加速大模型训练
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../quanto-diffusers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    基于 Quanto 和 Diffusers 的内存高效 transformer 扩散模型
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../quanto-introduction/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Quanto：PyTorch 量化工具包
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ram-efficient-pytorch-fsdp/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用 PyTorch FSDP 微调 Llama 2 70B
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../red-teaming/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    为大语言模型建立红队对抗
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Reformer 模型 - 突破语言建模的极限
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../regions/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    HF Hub 现已加入存储区域功能
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../researcher-dataset-sharing/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    在 Hugging Face Hub 分享你的开源数据集
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../rlhf/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ChatGPT 背后的“功臣”——RLHF 技术详解
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../rwkv/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    RWKV -- transformer 与 RNN 的强强联合
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ryght-case-study/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Ryght 在 Hugging Face 专家助力下赋能医疗保健和生命科学之旅
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../safecoder/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    推介 SafeCoder
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../sc2-instruct/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    StarCoder2-Instruct: 完全透明和可自我对齐的代码生成
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../sd3-5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    欢迎 Stable Diffusion 3.5 Large 加入 🧨 Diffusers
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../sd3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    欢迎 Stable Diffusion 3 加入 🧨 Diffusers
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../sd_distillation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    开源 SD-Small 和 SD-Tiny 知识蒸馏代码与权重
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../sdxl_lora_advanced_script/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    全世界 LoRA 训练脚本，联合起来!
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../setfit-absa/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    SetFitABSA：基于 SetFit 的少样本、方面级情感分析
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../setfit-optimum-intel/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    在英特尔至强 CPU 上使用 🤗 Optimum Intel 实现超快 SetFit 推理
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../setfit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    SetFit: 高效的无提示少样本学习
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../smollm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    SmolLM：一个超快速、超高性能的小模型集合
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../speecht5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用 SpeechT5 进行语音合成、识别和更多功能
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../sql-console/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    为数据集而生的 SQL 控制台
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../stable-diffusion-finetuning-intel/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    在英特尔 CPU 上微调 Stable Diffusion 模型
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../stable-diffusion-inference-intel/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    在英特尔 CPU 上加速 Stable Diffusion 推理
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../stable_diffusion/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用Diffusers来实现Stable Diffusion 🧨
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../stackllama/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    “StackLLaMA”: 用 RLHF 训练 LLaMA 的手把手教程
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../starchat-alpha/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用 StarCoder 创建一个编程助手
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../starcoder/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    StarCoder：最先进的代码大模型
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../starcoder2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    StarCoder2 及 The Stack v2 数据集正式发布
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../synthetic-data-save-costs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    合成数据：利用开源技术节约资金、时间和减少碳排放
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../synthid-text/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    SynthID Text：在 AI 生成文本中应用不可见水印的新技术
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../t2i-sdxl-adapters/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    在 SDXL 上用 T2I-Adapter 实现高效可控的文生图
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../text-to-video/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    深入理解文生视频模型
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../textgen-pipe-gaudi/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    基于英特尔® Gaudi® 2 AI 加速器的文本生成流水线
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tgi-benchmarking/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TGI 基准测试
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../the-age-of-ml-as-code/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    机器学习即代码的时代已经到来
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../the_n_implementation_details_of_rlhf_with_ppo/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用 PPO 算法进行 RLHF 的 N 步实现细节
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../time-series-transformers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用 🤗 Transformers 进行概率时间序列预测
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../train-dgx-cloud/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    在 NVIDIA DGX Cloud上使用 H100 GPU 轻松训练模型
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../train-optimize-sd-intel/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    基于 NNCF 和 🤗 Optimum 面向 Intel CPU 对 Stable Diffusion 优化
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../train-sentence-transformers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    用 Sentence Transformers v3 训练和微调嵌入模型
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../train-your-controlnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用 diffusers 训练你自己的 ControlNet 🧨
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../transformers-design-philosophy/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    〜不要〜重复自己
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../trl-ddpo/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用 DDPO 在 TRL 中微调 Stable Diffusion 模型
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../trl-peft/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    在一张 24 GB 的消费级显卡上用 RLHF 微调 20B LLMs
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../trufflesecurity-partnership/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Hugging Face 与 TruffleHog 成为合作伙伴，实现风险信息预警
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../unified-tool-use/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    对 LLM 工具使用进行统一
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../unity-api/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    如何安装和使用 Hugging Face Unity API
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../unity-asr/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    如何在 Unity 游戏中集成 AI 语音识别？
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../unity-in-spaces/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    如何在 🤗 Space 上托管 Unity 游戏
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../universal_assisted_generation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    通用辅助生成：使用任意辅助模型加速解码
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../vertex-colored-to-textured-mesh/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    顶点着色网格转换为 UV 映射的纹理化网格
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../vision_language_pretraining/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    深入了解视觉语言模型
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../vit-align/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Kakao Brain 的开源 ViT、ALIGN 和 COYO 文字
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../vlms/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    视觉语言模型详解
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../watermarking/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    人工智能水印技术入门：工具与技巧
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../whisper-speculative-decoding/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用推测解码使 Whisper 实现 2 倍的推理加速
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../winning-aimo-progress-prize/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    NuminaMath 是如何荣膺首届 AIMO 进步奖的？
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../xethub-joins-hf/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    XetHub 加入 Hugging Face!
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../zero-shot-vqa-docmatix/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LAVE：使用 LLM 对 Docmatix 进行零样本 VQA 评估 - 我们还需要微调吗？
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ops/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    运维
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
      
      
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../blog/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    博客
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../resume/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    简历模板
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
    <a href="https://github.com/C-L-STARK/C-L-STARK.github.io/edit/master/docs/ml/1_58_llm_extreme_quantization.md" title="Edit this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 3a2 2 0 0 1 2 2v14a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V5a2 2 0 0 1 2-2zm-2.3 6.35c.22-.21.22-.56 0-.77L15.42 7.3a.53.53 0 0 0-.77 0l-1 1 2.05 2.05zM7 14.94V17h2.06l6.06-6.06-2.06-2.06z"/></svg>
    </a>
  
  


<h1 id="llms-158">将 LLMs 精调至 1.58 比特：使极端量化变简单<a class="headerlink" href="#llms-158" title="Permanent link">&para;</a></h1>
<p>随着大语言模型（LLMs）规模和复杂性的增长，寻找减少它们的计算和能耗的方法已成为一个关键挑战。一种流行的解决方案是量化，其中参数的精度从标准的16位浮点（FP16）或32位浮点（FP32）降低到8位或4位等低位格式。虽然这种方法显著减少了内存使用量并加快了计算速度，但往往以准确性为代价。过度降低精度可能导致模型丢失关键信息，从而导致性能下降。</p>
<p><a href="https://arxiv.org/abs/2402.17764">BitNet</a>是一种特殊的transformers架构，它用仅三个值：<code>(-1, 0, 1)</code>表示每个参数，提供了每个参数仅为1.58 ( \( log_2(3) \) )比特的极端量化。然而，这需要从头开始训练一个模型。虽然结果令人印象深刻，但并非每个人都有预算来进行大语言模型的预训练。为了克服这一限制，我们探索了一些技巧，允许将现有模型精调至 1.58 比特！继续阅读以了解更多！</p>
<h2 id="_1">目录<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h2>
<ul>
<li><a href="#简介">简介</a></li>
<li><a href="#更深入地了解什么是BitNet">更深入地了解什么是BitNet</a></li>
<li><a href="#1.58比特的预训练结果">1.58 比特的预训练结果</a></li>
<li><a href="#1.58比特的微调">1.58 比特的微调</a></li>
<li><a href="#使用的算子和测试标准">使用的内核和测试标准</a></li>
<li><a href="#结论">结论</a></li>
<li><a href="#致谢">致谢</a></li>
<li><a href="#更多资源">更多资源</a></li>
</ul>
<h2 id="_2">简介<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h2>
<p><a href="https://arxiv.org/abs/2402.17764">BitNet</a>是由微软研究院提出的一种模型架构，其采用极端量化的方式，用仅三个值 -1、0 和 1 来表示每个参数。这导致模型每个参数仅使用1.58比特，显著降低了计算和内存需求。</p>
<p>该架构在执行矩阵乘法时使用INT8加法计算，这与以Llama为例的传统LLM架构的FP16乘加操作完全不同。</p>
<figure style="text-align: center;">
  <a class="glightbox" href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/matmulfree.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/matmulfree.png" alt="BitNet b1.58的新计算范式" style="width: 100%;"/></a>
  <figcaption>BitNet b1.58的新计算范式 (出处: BitNet论文 https://arxiv.org/abs/2402.17764)</figcaption>
</figure>

<p>这种方法在理论上降低能耗，与 Llama 基准相比，BitNet b1.58 在矩阵乘法方面节省了 71.4 倍的计算能耗。</p>
<figure style="text-align: center;">
  <a class="glightbox" href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/energy_consumption.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/energy_consumption.png" alt="BitNet b1.58与Llama的能耗对比" style="width: 100%;"/></a>
  <figcaption>BitNet b1.58与Llama的能耗对比 (出处: BitNet 论文 https://arxiv.org/abs/2402.17764)</figcaption>
</figure>

<p>我们成功地使用BitNet架构对<a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct">Llama3 8B model</a>模型进行了精调，在下游任务中取得了良好的性能。我们开发的 8B 模型由 <a href="https://huggingface.co/HF1BitLLM">HF1BitLLM</a>组织发布。其中两个模型在10B的token上进行了不同的训练设置的微调，而第三个模型在100B的token上进行了微调。值得注意的是，我们的模型在MMLU基准测试中超越了 Llama 1 7B 模型。</p>
<h3 id="transformers">如何在 Transformers 中使用<a class="headerlink" href="#transformers" title="Permanent link">&para;</a></h3>
<p>为了将BitNet架构集成到Transformers中，我们引入了一种名为"bitnet"的新量化方法（<a href="https://github.com/huggingface/transformers/pull/33410">PR</a>）。该方法涉及将标准的 Linear 层替换为专门设计用于 BitNet 架构的 BitLinear 层，其实现了相应的动态的激活量化、权重解包和矩阵乘法的操作。</p>
<p>在 Transformers 中加载和测试模型非常简单，API没有任何更改：
<div class="language-python highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span class="s2">&quot;HF1BitLLM/Llama3-8B-1.58-100B-tokens&quot;</span><span class="p">,</span>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span>
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span>
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a><span class="p">)</span>    
</span><span id="__span-0-6"><a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;</span><span class="p">)</span>
</span><span id="__span-0-7"><a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>
</span><span id="__span-0-8"><a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a><span class="n">input_text</span> <span class="o">=</span> <span class="s2">&quot;Daniel went back to the the the garden. Mary travelled to the kitchen. Sandra journeyed to the kitchen. Sandra went to the hallway. John went to the bedroom. Mary went back to the garden. Where is Mary?</span><span class="se">\n</span><span class="s2">Answer:&quot;</span>
</span><span id="__span-0-9"><a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a>
</span><span id="__span-0-10"><a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a><span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">input_text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
</span><span id="__span-0-11"><a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a><span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</span><span id="__span-0-12"><a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a><span class="n">generated_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-0-13"><a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a><span class="nb">print</span><span class="p">(</span><span class="n">generated_text</span><span class="p">)</span>
</span></code></pre></div>
通过这段代码，一切都直接在幕后完美地完成了，因此无需担心额外的复杂性，您只需要做的只是安装最新版本的transformers。</p>
<p>要快速测试模型，请查看这个 <a href="https://colab.research.google.com/drive/1ovmQUOtnYIdvcBkwEE4MzVL1HKfFHdNT?usp=sharing">notebook</a>。</p>
<h2 id="bitnet">更深入地了解什么是BitNet<a class="headerlink" href="#bitnet" title="Permanent link">&para;</a></h2>
<p><a href="https://arxiv.org/abs/2402.17764">BitNet</a> 在多头注意力和前馈网络中替换了传统的 Linear 层，使用了称为 BitLinear 的特殊层，这些层使用三值精度（甚至在初始版本中使用二值精度）。在这个项目中，我们使用的 BitLinear 层对权重使用三值精度（取值为 -1、0 和 1），并将激活量化为 8 位精度。我们在训练和推理中使用不同的 BitLinear 实现，接下来的部分将会介绍。</p>
<p>在三值精度训练中的主要障碍是权重值被离散化（通过<code>round()</code>函数），因此不可微分。BitLinear 通过一个巧妙的技巧解决了这个问题：<a href="https://arxiv.org/abs/1903.05662">STE (Straight Through Estimator)</a>。STE 允许梯度通过不可微分的取整操作，通过将其梯度近似为1（将<code>round()</code>视为等同于恒等函数）来实现。另一种观点是，STE 让梯度通过取整步骤，好像取整从未发生过一样，从而使用标准基于梯度的优化技术来更新权重。</p>
<figure style="text-align: center;">
  <a class="glightbox" href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/bitlinear.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/bitlinear.png" alt="使用BitLienar的BitNet模型架构" style="width: 100%"/></a>
  <figcaption>使用 BitLienar 的 BitNet 模型架构 (出处: BitNet 论文 https://arxiv.org/pdf/2310.11453)</figcaption>
</figure>

<h3 id="_3">训练<a class="headerlink" href="#_3" title="Permanent link">&para;</a></h3>
<p>我们在完整精度下进行训练，但在训练过程中将权重量化为三值，使用 per-tensor 的对称量化。首先，我们计算权重矩阵的绝对值的平均值，并将其用作 scale。然后，我们将权重除以 scale，对值进行取整，将其限制在 -1 和 1 的区间内，最后将权重其反量化回完整精度。</p>
<p>\( scale_w = \frac{1}{\frac{1}{nm} \sum_{ij} |W_{ij}|} \)</p>
<p>\( W_q = \text{clamp}_{[-1,1]}(\text{round}(W*scale)) \)</p>
<p>\( W_{dequantized} = W_q*scale_w \)</p>
<p>激活然后被量化为指定的比特宽度（在我们的情况下是8位），使用per-token的最大绝对值量化（要了解量化方法的全面介绍，请查看这篇<a href="https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html">post</a>）。这涉及将激活缩放到[-128, 127]的范围以适应8位比特宽度。量化公式如下：</p>
<p>\( scale_x = \frac{127}{|X|_{\text{max}, \, \text{dim}=-1}} \)</p>
<p>\( X_q = \text{clamp}_{[-128,127]}(\text{round}(X*scale)) \)</p>
<p>\( X_{dequantized} = X_q * scale_x \)</p>
<p>为了使这些公式更加清晰，下面是一些使用3x3的矩阵的权重和激活量化的例子：</p>
<hr />
<details>
  <summary>例子1：权重矩阵量化</summary>

  假设权重矩阵 \( W \) 为:


  \\(  W = 
  \begin{bmatrix}
  0.8 & -0.5 & 1.2 \\
  -1.5 & 0.4 & -0.9 \\
  1.3 & -0.7 & 0.2
  \end{bmatrix} \\)

  **第一步：计算权重的scale**

  使用公式：


  \\( scale_w = \frac{1}{\frac{1}{nm} \sum_{ij} |W_{ij}|} \\)

  我们计算 \( W \)激活值的平均值：


\\( \frac{1}{nm} \sum_{ij} |W_{ij}| = \frac{1}{9}(0.8 + 0.5 + 1.2 + 1.5 + 0.4 + 0.9 + 1.3 + 0.7 + 0.2) = \frac{1}{9}(7.5) = 0.8333 \\)

  现在得到的 scale 为：


  \\( scale_w = \frac{1}{0.8333} \approx 1.2 \\)

  **第二步：量化权重矩阵**

  使用公式：

 \\( W_q = \text{clamp}_{[-1, 1]}(\text{round}(W \times scale_w)) \\)

 我们首先将权重缩放\\( scale_w \approx 1.2 \\)倍:

\\( W \times scale_w = 
  \begin{bmatrix}
  0.8 \times 1.2 & -0.5 \times 1.2 & 1.2 \times 1.2 \\
  -1.5 \times 1.2 & 0.4 \times 1.2 & -0.9 \times 1.2 \\
  1.3 \times 1.2 & -0.7 \times 1.2 & 0.2 \times 1.2
  \end{bmatrix}
  =
  \begin{bmatrix}
  0.96 & -0.6 & 1.44 \\
  -1.8 & 0.48 & -1.08 \\
  1.56 & -0.84 & 0.24
  \end{bmatrix} \\)

  然后我们将其取整并截断到 \\( [-1, 1] \\)的区间内：

\\( W_q = 
  \begin{bmatrix}
  1 & -1 & 1 \\
  -1 & 0 & -1 \\
  1 & -1 & 0
  \end{bmatrix} \\)

**第三步：反量化权重**

  最后我们反量化该权重：


\\( W_{dequantized} = W_q \times scale_w \\)

  使用scale_w将权重恢复到原来的范围，我们可以得到：


 \\( W_{dequantized} = 
  \begin{bmatrix}
  1 \times 1.2 & -1 \times 1.2 & 1 \times 1.2 \\
  -1 \times 1.2 & 0 \times 1.2 & -1 \times 1.2 \\
  1 \times 1.2 & -1 \times 1.2 & 0 \times 1.2
  \end{bmatrix}
  =
  \begin{bmatrix}
  1.2 & -1.2 & 1.2 \\
  -1.2 & 0 & -1.2 \\
  1.2 & -1.2 & 0
  \end{bmatrix} \\)

</details>

<details>
  <summary>例子2：激活矩阵的量化</summary>

  假设激活矩阵\( X \)为：

  \\( X = 
  \begin{bmatrix}
  1.0 & -0.6 & 0.7 \\
  -0.9 & 0.4 & -1.2 \\
  0.8 & -0.5 & 0.3
  \end{bmatrix} \\) 

  **第一步：计算激活的 scale**

  对于每一行（或者通道），计算其最大的绝对值

 - **第1行**：最大绝对值 = 1.0
 - **第2行**：最大绝对值 = 1.2
 - **第3行**：最大绝对值 = 0.8

  计算每行的 scale：

  \\( \text{scale} = \begin{bmatrix}
  \frac{127}{1.0} \\
  \frac{127}{1.2} \\
  \frac{127}{0.8}
  \end{bmatrix}
  =
  \begin{bmatrix}
  127 \\
  105.83 \\
  158.75
  \end{bmatrix} \\)

  **步骤2：量化激活矩阵**  

  使用以下公式：

  \\( X_q = \text{clamp}_{[-128,127]}(\text{round}(X \times \text{scale})) \\)

  缩放相应的激活值：

  \\( X \times \text{scale} = 
  \begin{bmatrix}
  1.0 \times 127 & -0.6 \times 127 & 0.7 \times 127 \\
  -0.9 \times 105.83 & 0.4 \times 105.83 & -1.2 \times 105.83 \\
  0.8 \times 158.75 & -0.5 \times 158.75 & 0.3 \times 158.75
  \end{bmatrix}
  =
  \begin{bmatrix}
  127 & -76.2 & 88.9 \\
  -95.2 & 42.3 & -127 \\
  127 & -79.4 & 47.6
  \end{bmatrix} \\)

  将值取整并截断在\\([-128, 127] \\)的范围内：

  \\( X_q = 
  \begin{bmatrix}
  127 & -76 & 89 \\
  -95 & 42 & -127 \\
  127 & -79 & 48
  \end{bmatrix} \\)

  **第三步：反量化激活**

  最后我们反量化激活值：

  \\( X_{dequantized} = X_q \times \frac{1}{\text{scale}} \\)


  使用 scale 对值进行恢复：

  \\( X_{dequantized} = 
  \begin{bmatrix}
  127 \times \frac{1}{127} & -76 \times \frac{1}{127} & 89 \times \frac{1}{127} \\
  -95 \times \frac{1}{105.83} & 42 \times \frac{1}{105.83} & -127 \times \frac{1}{105.83} \\
  127 \times \frac{1}{158.75} & -79 \times \frac{1}{158.75} & 48 \times \frac{1}{158.75}
  \end{bmatrix}
  =
  \begin{bmatrix}
  1.0 & -0.6 & 0.7 \\
  -0.9 & 0.4 & -1.2 \\
  0.8 & -0.5 & 0.3
  \end{bmatrix} \\)


</details>

<hr />
<p>我们在量化激活之前使用层归一化（Layer Normalization，LN）以保持输出的方差：</p>
<p>\( \text{LN}(x) = \frac{x - E(x)}{\sqrt{\text{Var}(x) + \epsilon}} \)</p>
<p>这里ε是防止溢出的一个非常小的值</p>
<p>如前所述，<code>round()</code>函数是不可微分的。我们使用<code>detach()</code>作为一个技巧，在反向传播中实现可微分的STE（Straight-Through Estimator）：</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-1-1"><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a><span class="c1"># Adapted from https://github.com/microsoft/unilm/blob/master/bitnet/The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ.pdf</span>
</span><span id="__span-1-2"><a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a><span class="kn">import</span> <span class="nn">torch</span>
</span><span id="__span-1-3"><a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span> 
</span><span id="__span-1-4"><a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
</span><span id="__span-1-5"><a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a>
</span><span id="__span-1-6"><a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a><span class="k">def</span> <span class="nf">activation_quant</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
</span><span id="__span-1-7"><a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a>    <span class="n">scale</span> <span class="o">=</span> <span class="mf">127.0</span> <span class="o">/</span> <span class="n">x</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">clamp_</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
</span><span id="__span-1-8"><a id="__codelineno-1-8" name="__codelineno-1-8" href="#__codelineno-1-8"></a>    <span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">scale</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">()</span><span class="o">.</span><span class="n">clamp_</span><span class="p">(</span><span class="o">-</span><span class="mi">128</span><span class="p">,</span> <span class="mi">127</span><span class="p">)</span> <span class="o">/</span> <span class="n">scale</span>
</span><span id="__span-1-9"><a id="__codelineno-1-9" name="__codelineno-1-9" href="#__codelineno-1-9"></a>    <span class="k">return</span> <span class="n">y</span>
</span><span id="__span-1-10"><a id="__codelineno-1-10" name="__codelineno-1-10" href="#__codelineno-1-10"></a>
</span><span id="__span-1-11"><a id="__codelineno-1-11" name="__codelineno-1-11" href="#__codelineno-1-11"></a><span class="k">def</span> <span class="nf">weight_quant</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
</span><span id="__span-1-12"><a id="__codelineno-1-12" name="__codelineno-1-12" href="#__codelineno-1-12"></a>    <span class="n">scale</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">w</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">clamp_</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
</span><span id="__span-1-13"><a id="__codelineno-1-13" name="__codelineno-1-13" href="#__codelineno-1-13"></a>    <span class="n">u</span> <span class="o">=</span> <span class="p">(</span><span class="n">w</span> <span class="o">*</span> <span class="n">scale</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">()</span><span class="o">.</span><span class="n">clamp_</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">scale</span>
</span><span id="__span-1-14"><a id="__codelineno-1-14" name="__codelineno-1-14" href="#__codelineno-1-14"></a>    <span class="k">return</span> <span class="n">u</span>
</span><span id="__span-1-15"><a id="__codelineno-1-15" name="__codelineno-1-15" href="#__codelineno-1-15"></a>
</span><span id="__span-1-16"><a id="__codelineno-1-16" name="__codelineno-1-16" href="#__codelineno-1-16"></a><span class="k">class</span> <span class="nc">BitLinear</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
</span><span id="__span-1-17"><a id="__codelineno-1-17" name="__codelineno-1-17" href="#__codelineno-1-17"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-1-18"><a id="__codelineno-1-18" name="__codelineno-1-18" href="#__codelineno-1-18"></a><span class="sd">    Only for training</span>
</span><span id="__span-1-19"><a id="__codelineno-1-19" name="__codelineno-1-19" href="#__codelineno-1-19"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-1-20"><a id="__codelineno-1-20" name="__codelineno-1-20" href="#__codelineno-1-20"></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="__span-1-21"><a id="__codelineno-1-21" name="__codelineno-1-21" href="#__codelineno-1-21"></a>        <span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span>
</span><span id="__span-1-22"><a id="__codelineno-1-22" name="__codelineno-1-22" href="#__codelineno-1-22"></a>        <span class="n">x_norm</span> <span class="o">=</span> <span class="n">LN</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="__span-1-23"><a id="__codelineno-1-23" name="__codelineno-1-23" href="#__codelineno-1-23"></a>
</span><span id="__span-1-24"><a id="__codelineno-1-24" name="__codelineno-1-24" href="#__codelineno-1-24"></a>        <span class="c1"># A trick for implementing Straight−Through−Estimator (STE) using detach()</span>
</span><span id="__span-1-25"><a id="__codelineno-1-25" name="__codelineno-1-25" href="#__codelineno-1-25"></a>        <span class="n">x_quant</span> <span class="o">=</span> <span class="n">x_norm</span> <span class="o">+</span> <span class="p">(</span><span class="n">activation_quant</span><span class="p">(</span><span class="n">x_norm</span><span class="p">)</span> <span class="o">-</span> <span class="n">x_norm</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="__span-1-26"><a id="__codelineno-1-26" name="__codelineno-1-26" href="#__codelineno-1-26"></a>        <span class="n">w_quant</span> <span class="o">=</span> <span class="n">w</span> <span class="o">+</span> <span class="p">(</span><span class="n">weight_quant</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">-</span> <span class="n">w</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="__span-1-27"><a id="__codelineno-1-27" name="__codelineno-1-27" href="#__codelineno-1-27"></a>
</span><span id="__span-1-28"><a id="__codelineno-1-28" name="__codelineno-1-28" href="#__codelineno-1-28"></a>        <span class="c1"># Perform quantized linear transformation</span>
</span><span id="__span-1-29"><a id="__codelineno-1-29" name="__codelineno-1-29" href="#__codelineno-1-29"></a>        <span class="n">y</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x_quant</span><span class="p">,</span> <span class="n">w_quant</span><span class="p">)</span>
</span><span id="__span-1-30"><a id="__codelineno-1-30" name="__codelineno-1-30" href="#__codelineno-1-30"></a>        <span class="k">return</span> <span class="n">y</span>
</span></code></pre></div>
<h3 id="_4">推理<a class="headerlink" href="#_4" title="Permanent link">&para;</a></h3>
<p>在推理过程中，我们只是将权重量化为三值，而不重新反量化。我们对激活采用相同的方法，使用8位精度，然后使用高效的算子执行矩阵乘法，接着通过权重和激活的 scale 进行除法。这能够显著提高推理的速度，特别是在优化的硬件上。您可以看到，在训练期间反量化的过程与推理不同，因为矩阵乘法保持在fp16/bf16/fp32中以进行正确的训练。</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-2-1"><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a><span class="c1"># Adapted from https://github.com/microsoft/unilm/blob/master/bitnet/The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ.pdf</span>
</span><span id="__span-2-2"><a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a><span class="kn">import</span> <span class="nn">torch</span>
</span><span id="__span-2-3"><a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span> 
</span><span id="__span-2-4"><a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
</span><span id="__span-2-5"><a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a>
</span><span id="__span-2-6"><a id="__codelineno-2-6" name="__codelineno-2-6" href="#__codelineno-2-6"></a><span class="k">def</span> <span class="nf">activation_quant_inference</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
</span><span id="__span-2-7"><a id="__codelineno-2-7" name="__codelineno-2-7" href="#__codelineno-2-7"></a>    <span class="n">x</span> <span class="o">=</span> <span class="n">LN</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="__span-2-8"><a id="__codelineno-2-8" name="__codelineno-2-8" href="#__codelineno-2-8"></a>    <span class="n">scale</span> <span class="o">=</span> <span class="mf">127.0</span> <span class="o">/</span> <span class="n">x</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">clamp_</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
</span><span id="__span-2-9"><a id="__codelineno-2-9" name="__codelineno-2-9" href="#__codelineno-2-9"></a>    <span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">scale</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">()</span><span class="o">.</span><span class="n">clamp_</span><span class="p">(</span><span class="o">-</span><span class="mi">128</span><span class="p">,</span> <span class="mi">127</span><span class="p">)</span>
</span><span id="__span-2-10"><a id="__codelineno-2-10" name="__codelineno-2-10" href="#__codelineno-2-10"></a>    <span class="k">return</span> <span class="n">y</span><span class="p">,</span> <span class="n">scale</span>
</span><span id="__span-2-11"><a id="__codelineno-2-11" name="__codelineno-2-11" href="#__codelineno-2-11"></a>
</span><span id="__span-2-12"><a id="__codelineno-2-12" name="__codelineno-2-12" href="#__codelineno-2-12"></a><span class="k">class</span> <span class="nc">BitLinear</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
</span><span id="__span-2-13"><a id="__codelineno-2-13" name="__codelineno-2-13" href="#__codelineno-2-13"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-2-14"><a id="__codelineno-2-14" name="__codelineno-2-14" href="#__codelineno-2-14"></a><span class="sd">    Only for training</span>
</span><span id="__span-2-15"><a id="__codelineno-2-15" name="__codelineno-2-15" href="#__codelineno-2-15"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-2-16"><a id="__codelineno-2-16" name="__codelineno-2-16" href="#__codelineno-2-16"></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="__span-2-17"><a id="__codelineno-2-17" name="__codelineno-2-17" href="#__codelineno-2-17"></a>        <span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="c1"># weights here are already quantized to (-1, 0, 1)    </span>
</span><span id="__span-2-18"><a id="__codelineno-2-18" name="__codelineno-2-18" href="#__codelineno-2-18"></a>        <span class="n">w_scale</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_scale</span>  
</span><span id="__span-2-19"><a id="__codelineno-2-19" name="__codelineno-2-19" href="#__codelineno-2-19"></a>        <span class="n">x_quant</span><span class="p">,</span> <span class="n">x_scale</span> <span class="o">=</span> <span class="n">activation_quant_inference</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="__span-2-20"><a id="__codelineno-2-20" name="__codelineno-2-20" href="#__codelineno-2-20"></a>        <span class="n">y</span> <span class="o">=</span> <span class="n">efficient_kernel</span><span class="p">(</span><span class="n">x_quant</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="o">/</span> <span class="n">w_scale</span> <span class="o">/</span> <span class="n">x_scale</span>
</span><span id="__span-2-21"><a id="__codelineno-2-21" name="__codelineno-2-21" href="#__codelineno-2-21"></a>        <span class="k">return</span> <span class="n">y</span>
</span></code></pre></div>
<h2 id="158">1.58比特的预训练结果<a class="headerlink" href="#158" title="Permanent link">&para;</a></h2>
<p>在尝试微调之前，我们首先尝试复现 BitNet 论文中关于预训练的结果。我们使用了一个小数据集<a href="https://huggingface.co/datasets/roneneldan/TinyStories">tinystories</a>，以及一个<a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct">Llama3 8B模型</a>。我们发现，像论文中所做的那样添加归一化函数会提高性能。例如，在训练2000步之后，我们在验证集上的困惑度，没有归一化时为 6.3，使用归一化后为 5.9。在这两种情况下，训练都是稳定的。</p>
<figure style="text-align: center;">
  <a class="glightbox" href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/pre-training.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/pre-training.png" alt="在有层归一化（蓝色）和没有（橙色）的预训练图像" style="width: 100%;"/></a>
  <figcaption> 在有层归一化（蓝色）和没有（橙色）的预训练图像 </figcaption>
</figure>

<p>虽然这种方法在预训练中看起来非常有趣，但只有少数机构能够负担大规模的预训练。然而，因为存在有大量强大的预训练模型，如果它们可以在预训练后转换为 1.58 位，将会非常有用。其他小组曾报告称，微调的结果不如预训练取得的结果那么强大，因此我们展开了研究，看看我们是否能够让 1.58 比特地微调起作用。</p>
<h2 id="158_1">1.58比特的微调<a class="headerlink" href="#158_1" title="Permanent link">&para;</a></h2>
<p>当我们从预训练的 Llama3 8B 权重开始微调时，模型表现略有提高，但并不如我们预期的那么好。</p>
<blockquote>
<p><strong>Note:</strong> 所有的实验都在<a href="https://github.com/huggingface/nanotron">Nanotron</a>上进行，如果您对尝试1.58位的预训练或微调感兴趣，可以查看这个<a href="https://github.com/huggingface/nanotron/pull/180">PR链接</a>。</p>
</blockquote>
<figure style="text-align: center;">
  <a class="glightbox" href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/finetuning_basic.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/finetuning_basic.png" alt="微调曲线对比预训练曲线" style="width: 100%;"/></a>
  <figcaption> 微调曲线对比预训练曲线 </figcaption>
</figure>

<p>为了理解原因，我们尝试检查随机初始化模型和预训练模型的权重分布，以确定可能的问题。</p>
<div style="display: flex; justify-content: center;">
  <figure style="margin-right: 20px; text-align: center;">
    <a class="glightbox" href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/poids_aléatoires.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/poids_aléatoires.png" alt="随机的权重分布（合并的标准差为2）" style="width: 400px;" /></a>
    <figcaption> 随机的权重分布（合并的标准差为 2）</figcaption>
  </figure>
  <figure style="text-align: center;">
    <a class="glightbox" href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/poids_llama3.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/poids_llama3.png" alt="预训练Llama3的权重分布" style="width: 400px;" /></a>
    <figcaption>预训练Llama3的权重分布</figcaption>
  </figure>
</div>

<p>两个分布的scale分别为：</p>
<div style="display: flex; justify-content: center;">
  <figure style="margin-right: 20px; text-align: center;">
    <a class="glightbox" href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/scales_random.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/scales_random.png" alt="随机权重的scale分布" style="width: 400px;" /></a>
    <figcaption> 随机权重的scale分布 </figcaption>
  </figure>
  <figure style="text-align: center;">
    <a class="glightbox" href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/scales_llama3.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/scales_llama3.png" alt="预训练权重的scale分布" style="width: 400px;" /></a>
    <figcaption> 预训练权重的scale分布 </figcaption>
  </figure>
</div>

<p>初始随机权重分布是两个正态分布的混合：</p>
<ul>
<li>一个标准差为 \( 0.025 \)</li>
<li>另一个标准差为 \( \frac{0.025}{\sqrt{2 \cdot \text{num_hidden_layers}}} = 0.00325 \)</li>
</ul>
<p>这是因为在<code>nanotron</code>中对列线性权重和行线性权重使用了不同的标准差。在量化版本中，所有矩阵只有两个权重尺度（50.25和402），这两个尺度分别是每个矩阵权重的绝对值的倒数的平均值：<code>scale = 1.0 / w.abs().mean().clamp_(min=1e-5)</code></p>
<ul>
<li>对于 \(\text{scale} = 50.25 \)，\( w.abs().mean() = 0.0199 \)，导致 \(\text{std} = 0.025 \)，与我们的第一个标准差相匹配。用于推导标准差的公式基于 \( |w| \) 的半正态分布的期望：<br />
  \( \mathbb{E}(|w|) = \text{std}(w) \cdot \sqrt{\frac{2}{\pi}} \)</li>
<li>对于 \(\text{scale} = 402 \)，\( w.abs().mean() = 0.0025 \)，导致 \(\text{std} = 0.00325 \)</li>
</ul>
<p>另一方面，预训练权重的分布看起来像是一个标准差为 \(0.013\) 的正态分布。</p>
<p>显然，预训练模型从更多信息（scale）开始，而随机初始化的模型从实际上没有信息开始，并随着时间逐渐增加信息。我们的结论是，从随机权重开始给予模型最小的初始信息，从而实现逐步学习过程，而在微调期间，引入BitLinear层会使模型丧失所有先前的信息。</p>
<p>为了改善微调结果，我们尝试了不同的技术。例如，我们尝试过使用 per-row 和 per-column 量化而不是 per-tensor 量化，以保留更多来自Llama 3权重的信息。我们还尝试改变尺度计算的方式：不再仅仅将权重的平均绝对值作为尺度，而是将异常值（超过k倍平均绝对值的值，其中k是我们在实验中尝试变化的常数）的平均绝对值作为尺度，但我们并没有注意到明显的改善。</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-3-1"><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a><span class="k">def</span> <span class="nf">scale_outliers</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">threshold_factor</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
</span><span id="__span-3-2"><a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a>    <span class="n">mean_absolute_value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">tensor</span><span class="p">))</span>
</span><span id="__span-3-3"><a id="__codelineno-3-3" name="__codelineno-3-3" href="#__codelineno-3-3"></a>    <span class="n">threshold</span> <span class="o">=</span> <span class="n">threshold_factor</span> <span class="o">*</span> <span class="n">mean_absolute_value</span>
</span><span id="__span-3-4"><a id="__codelineno-3-4" name="__codelineno-3-4" href="#__codelineno-3-4"></a>    <span class="n">outliers</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">]</span>
</span><span id="__span-3-5"><a id="__codelineno-3-5" name="__codelineno-3-5" href="#__codelineno-3-5"></a>    <span class="n">mean_outlier_value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">outliers</span><span class="p">))</span>
</span><span id="__span-3-6"><a id="__codelineno-3-6" name="__codelineno-3-6" href="#__codelineno-3-6"></a>    <span class="k">return</span> <span class="n">mean_outlier_value</span>
</span><span id="__span-3-7"><a id="__codelineno-3-7" name="__codelineno-3-7" href="#__codelineno-3-7"></a>
</span><span id="__span-3-8"><a id="__codelineno-3-8" name="__codelineno-3-8" href="#__codelineno-3-8"></a><span class="k">def</span> <span class="nf">weight_quant_scaling</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
</span><span id="__span-3-9"><a id="__codelineno-3-9" name="__codelineno-3-9" href="#__codelineno-3-9"></a>    <span class="n">scale</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">scale_outliers</span><span class="p">(</span><span class="n">w</span><span class="p">)</span><span class="o">.</span><span class="n">clamp_</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
</span><span id="__span-3-10"><a id="__codelineno-3-10" name="__codelineno-3-10" href="#__codelineno-3-10"></a>    <span class="n">quantized_weights</span> <span class="o">=</span> <span class="p">(</span><span class="n">w</span> <span class="o">*</span> <span class="n">scale</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">()</span><span class="o">.</span><span class="n">clamp_</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">scale</span>
</span><span id="__span-3-11"><a id="__codelineno-3-11" name="__codelineno-3-11" href="#__codelineno-3-11"></a>    <span class="k">return</span> <span class="n">quantized_weights</span>
</span></code></pre></div>
<p>我们观察到，随机权重和 Llama 3 权重在损失开始时的数值约为13，这表明当引入量化时，Llama 3模型失去了所有先前的信息。为了进一步研究模型在这个过程中失去了多少信息，我们尝试了 per-group 量化。</p>
<p>作为一个合理性检查，我们首先将 group 大小设置为 1，这基本上意味着没有量化。在这种情况下，损失从 1.45 开始，与正常微调时的情况相同。然而，当我们将组大小增加到 2时，损失跳升到大约 11。这表明即使组大小最小为 2，模型仍几乎失去了所有信息。</p>
<p>为了解决这个问题，我们考虑逐渐引入量化而不是突然将其应用于每个张量的权重和激活。为了实现这一点，我们引入了一个 lambda 值来控制这个过程：</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-4-1"><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a><span class="n">lambda_</span> <span class="o">=</span> <span class="err">?</span>
</span><span id="__span-4-2"><a id="__codelineno-4-2" name="__codelineno-4-2" href="#__codelineno-4-2"></a><span class="n">x_quant</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">lambda_</span> <span class="o">*</span> <span class="p">(</span><span class="n">activation_quant</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="__span-4-3"><a id="__codelineno-4-3" name="__codelineno-4-3" href="#__codelineno-4-3"></a><span class="n">w_quant</span> <span class="o">=</span> <span class="n">w</span> <span class="o">+</span> <span class="n">lambda_</span> <span class="o">*</span> <span class="p">(</span><span class="n">weight_quant</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">-</span> <span class="n">w</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span></code></pre></div>
<p>当<code>lambda</code>设置为0是, 实际上没有量化发生, 当<code>lambda=1</code>时, 将应用完全的量化.</p>
<p>我们最初测试了一些离散的 lambda 值，比如 0.25、0.5、0.75 和 1。然而，这种方法并没有在结果上带来显著的改善，主要是因为 lambda=0.25 已经足够高，使损失开始得很高。</p>
<figure style="text-align: center;">
  <a class="glightbox" href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/lambda_0.25.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/lambda_0.25.png" alt="当lambda = 0.25-></a>0.5->0.75->1时的微调图像" style="width: 100%;"/>
  <figcaption> 当lambda = 0.25->0.5->0.75->1时的微调图像 </figcaption>
</figure>

<p>因此，我们决定尝试一个根据训练步骤动态调整的 <code>lambda</code> 值。</p>
<p>使用这种动态的 <code>lambda</code> 值导致更好的损失收敛，但在推理过程中，当 <code>lambda</code> 设置为 1 时，困惑度（perplexity或者ppl）的结果仍然远非令人满意。我们意识到这很可能是因为模型在 <code>lambda=1</code> 的情况下还没有受过足够长时间的训练。为了解决这个问题，我们调整了我们的 <code>lambda</code> 值来改善训练过程。</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-5-1"><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a><span class="n">lambda_</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">training_step</span> <span class="o">/</span> <span class="n">total_training_steps</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></code></pre></div>
<p>在这种配置下，经过 2000 步之后，我们有:</p>
<figure style="text-align: center;">
  <a class="glightbox" href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/lambda_training_step.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/lambda_training_step.png" alt="lambda = min(2*training_step/total_training_steps, 1)时的微调图像" style="width: 100%;"/></a>
  <figcaption> lambda = min(2*training_step/total_training_steps, 1)时的微调图像 </figcaption>
</figure>

<p>我们的微调方法整体上显示出更好的收敛性。你可以观察到在大约 1000 步时损失曲线略微增加，这对应于我们开始接近 <code>lambda=1</code> 或完全量化的时候。然而，在这一点之后，损失立即开始再次收敛，导致困惑度约为 4，得到了改善。</p>
<p>尽管取得了进展，但当我们在 WikiText 数据集上测试量化模型（而不是我们用于微调的 tinystories 数据集）时，困惑度非常高。这表明在特定数据集上以低比特模式微调模型会导致其丧失大部分通用知识。这个问题可能是因为我们在三值权重中追求的最小表示在不同数据集之间可能会有显著差异。为解决这个问题，我们扩展了我们的训练过程，包括了更大的<a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb">FineWeb-edu</a> 数据集。我们保持了一个 <code>lambda</code> 值为:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-6-1"><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a><span class="n">lambda_</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">training_step</span><span class="o">/</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></code></pre></div>
<p>我们选择了这个 <code>lambda</code> 值，因为它似乎是对模型进行warmup的一个很好的起点。然后，我们在 FineWeb-edu 数据集上使用学习率为 1e-4，训练了5000步。训练过程中使用了一个批量大小（BS）为 2B，总共训练了10B个token。</p>
<p>找到合适的学习率和合适的衰减率是具有挑战性的；这似乎是模型性能的一个关键因素。</p>
<figure style="text-align: center;">
  <a class="glightbox" href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/fineweb-edu.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/fineweb-edu.png" alt="在Fineweb-edu上进行warmup量化时的微调图像" style="width: 100%;"/></a>
  <figcaption> 在Fineweb-edu上进行warmup量化时的微调图像 </figcaption>
</figure>

<p>在 FineWeb-Edu上微调后，在 WikiText 数据集上达到 12.2 的困惑度是相当令人印象深刻的，考虑到我们只使用了 100 亿个标记。其他评估指标也显示出了强大的性能，考虑到数据量有限（请参见结果）。</p>
<p>尝试平滑 lambda 接近1时的急剧增加也是一个不错的想法。为了实现这一点，考虑使用 lambda 调度器，这些调度器在开始时呈指数增长，然后在接近 1 时趋于平稳。这种方法可以帮助模型更平稳地适应 lambda 值的变化，避免突然的波动。</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-7-1"><a id="__codelineno-7-1" name="__codelineno-7-1" href="#__codelineno-7-1"></a><span class="k">def</span> <span class="nf">scheduler</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="n">total_steps</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
</span><span id="__span-7-2"><a id="__codelineno-7-2" name="__codelineno-7-2" href="#__codelineno-7-2"></a>    <span class="n">normalized_step</span> <span class="o">=</span> <span class="n">step</span> <span class="o">/</span> <span class="n">total_steps</span>
</span><span id="__span-7-3"><a id="__codelineno-7-3" name="__codelineno-7-3" href="#__codelineno-7-3"></a>    <span class="k">return</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">normalized_step</span><span class="p">)</span><span class="o">**</span><span class="n">k</span>
</span></code></pre></div>
<p>对于不同的 k 值，总预热步数为 1，我们有如下图表：</p>
<figure style="text-align: center;">
  <a class="glightbox" href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/exp_scheduler.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/exp_scheduler.png" alt="不同k值时的指数调度器" style="width: 100%;"/></a>
  <figcaption>不同k值时的指数调度器</figcaption>
</figure>

<p>我们使用表现最好的学习率 1e-4进行了4次实验, 测试的k值分别为4, 6, 8, 10.</p>
<figure style="text-align: center;">
  <a class="glightbox" href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/exp_scheduler_results.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/exp_scheduler_results.png" alt="使用不同指数调度器时的微调图像" style="width: 100%;"/></a>
  <figcaption>使用不同指数调度器时的微调图像</figcaption>
</figure>

<p>平滑效果很好，不像线性调度器那样出现尖峰。然而，困惑度并不理想，大约保持在 15 左右，对下游任务的表现也没有改善。</p>
<p>我们还注意到了开始时的尖峰，模型难以从中恢复。当 lambda = 0 时，基本上没有量化，所以损失开始很低，大约在 2 左右。但在第一步之后，出现了一个尖峰，类似于线性调度器的情况（如上面的蓝色图表所示）。因此，我们尝试了另一种调度器即 Sigmoid 调度器，它开始缓慢上升，迅速上升到 1，然后在接近 1 时趋于稳定。</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-8-1"><a id="__codelineno-8-1" name="__codelineno-8-1" href="#__codelineno-8-1"></a><span class="k">def</span> <span class="nf">sigmoid_scheduler</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="n">total_steps</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
</span><span id="__span-8-2"><a id="__codelineno-8-2" name="__codelineno-8-2" href="#__codelineno-8-2"></a>    <span class="c1"># Sigmoid-like curve: slow start, fast middle, slow end</span>
</span><span id="__span-8-3"><a id="__codelineno-8-3" name="__codelineno-8-3" href="#__codelineno-8-3"></a>    <span class="n">normalized_step</span> <span class="o">=</span> <span class="n">step</span> <span class="o">/</span> <span class="n">total_steps</span>
</span><span id="__span-8-4"><a id="__codelineno-8-4" name="__codelineno-8-4" href="#__codelineno-8-4"></a>    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">k</span> <span class="o">*</span> <span class="p">(</span><span class="n">normalized_step</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)))</span>
</span></code></pre></div>
<p>对于不同的k值有以下的曲线:</p>
<figure style="text-align: center;">
  <a class="glightbox" href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/sig_scheduler.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/sig_scheduler.png" alt="对于不同k值的Sigmoid调度器" style="width: 100%;"/></a>
  <figcaption>对于不同k值的Sigmoid调度器</figcaption>
</figure>

<p>我们这次在k为15,20,25,40和100时进行了实验:</p>
<figure style="text-align: center;">
  <a class="glightbox" href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/sig_scheduler_exps.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/sig_scheduler_exps.png" alt="使用Sigmoid调度器进行微调的图像" style="width: 100%;"/></a>
  <figcaption> 使用Sigmoid调度器进行微调的图像 </figcaption>
</figure>

<p>lambda 的急剧增加导致在第 500 步左右出现不稳定，并没有解决第一次发散问题。然而，对于 <span class="arithmatex">\( k = 100 \)</span>，我们观察到在下游任务中有一些改善（请参阅结果表），尽管困惑度仍保持在 13.5 左右。尽管如此，与线性调度器相比，并没有显示明显的性能提升。</p>
<p>此外，我们尝试了使用随机权重和各种学习率从头开始训练模型的实验。这使我们能够比较我们的微调方法与传统的预训练方法的有效性。</p>
<figure style="text-align: center;">
  <a class="glightbox" href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/exp-randoms.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/exp-randoms.png" alt="不同学习率时的训练图像" style="width: 100%;"/></a>
  <figcaption>不同学习率时的训练图像</figcaption>
</figure>

<p>所有从随机权重训练的模型都没有比我们的微调模型表现更好。我们在这些模型中实现的最佳困惑度为 26，与我们的微调方法的结果相比略逊一筹。</p>
<h3 id="100btoken">扩展到100B个token!<a class="headerlink" href="#100btoken" title="Permanent link">&para;</a></h3>
<p>我们将实验扩展到了100B个token，以查看是否能够达到 Llama 3 8B 模型的性能水平。我们进行了更长时间的训练运行，从较短运行中表现最佳的检查点开始，使用线性调度器，并持续微调了 45,000 步。我们尝试了不同的学习率，虽然在某些指标上模型的表现接近 Llama 3 模型，但平均而言，仍然落后一些。</p>
<p>这里是我们在训练过程中在不同checkpoint评估的一些指标的例子：</p>
<figure style="text-align: center;">
  <a class="glightbox" href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/metrics_100B.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/metrics_100B.png" alt="在训练中不同学习率的多个指标评估结果" style="width: 100%;"/></a>
  <figcaption> 在训练中不同学习率的多个指标评估结果 </figcaption>
</figure>

<p>平均的分数如下:</p>
<figure style="text-align: center;">
  <a class="glightbox" href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/metric_avg.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/metric_avg.png" alt="在训练中不同学习率的平均评估结果" style="width: 100%;"/></a>
  <figcaption> 在训练中不同学习率的平均评估结果 </figcaption>
</figure>

<h3 id="_5">在更小的模型上的实验<a class="headerlink" href="#_5" title="Permanent link">&para;</a></h3>
<p>在我们对 SmolLM 等较小模型进行的初始实验中，我们观察到warmup量化技术并没有像对较大模型那样带来太多改进。这表明warmup量化的有效性可能与模型的大小和复杂性更密切相关。</p>
<p>例如，这里是 <a href="https://huggingface.co/HuggingFaceTB/SmolLM-135M">SmolLM 135M</a> 模型的损失曲线，比较了从一开始就使用warmup量化和完全量化的情况。有趣的是，这些曲线非常接近，得到的困惑度并没有显著不同。</p>
<figure style="text-align: center;">
  <a class="glightbox" href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/smol_llm_exp.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/smol_llm_exp.png" alt="有warmup量化和没有时的Smoll LLM微调实验" style="width: 100%;"/></a>
  <figcaption> 有warmup量化和没有时的Smoll LLM微调实验 </figcaption>
</figure>

<h3 id="_6">对比与结论<a class="headerlink" href="#_6" title="Permanent link">&para;</a></h3>
<p>BitNet 在与基准方法相比表现出色，特别是在较低比特数情况下。根据论文，BitNet 实现了与 8 位模型相当的分数，但推理成本显著更低。在 4 位模型的情况下，仅量化权重的方法胜过同时量化权重和激活的方法，因为激活更难量化。然而，使用 1.58 位权重的 BitNet 超越了仅权重和权重与激活量化方法。</p>
<p>下表展示了在 Llama3 8B 的 10B个token 微调过程之后各种指标的结果。这些结果与其他模型架构的结果进行了比较，以提供对性能的全面概述（所有评估均使用 <a href="https://github.com/huggingface/lighteval">Lighteval</a> 在 <a href="https://github.com/huggingface/nanotron">Nanotron</a> 格式模型上进行）。</p>
<figure style="text-align: center;">
  <a class="glightbox" href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/metrics_comparison_updated.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/metrics_comparison_updated.png" alt="与 Llama 模型的指标比较" style="width: 100%;"/></a>
  <figcaption>与 Llama 模型的指标比较：线性表示线性lambda调度器，Sigmoid表示 Sigmoid调度器（在我们的情况下 k = 100）</figcaption>
</figure>

<p>在仅使用三值权重进行 10B 个 token 微调后，该模型展现出令人印象深刻的性能，特别是与经历了更加广泛训练的其他模型相比。例如，它胜过了在数据集规模显著大得多的100B个token上训练的 Bitnet 7B 模型。此外，它的表现也优于 FBI LLM（Fully Binarized LLM）模型，后者在更庞大的 1.26T 个 token 上进行了蒸馏。这突显了该模型的效率和有效性，尽管其微调过程相对规模较小。</p>
<p>对于 100B 个 token 的实验，我们拥有的表现最佳的checkpoint如下：</p>
<figure style="text-align: center;">
  <a class="glightbox" href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/metrics_100B_table.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/metrics_100B_table.png" alt="100B个token微调后与 Llama 模型的指标比较" style="width: 100%;"/></a>
  <figcaption>100B个token微调后与 Llama 模型的指标比较</figcaption>
</figure>

<p>要复制这些结果，您可以查看这个<a href="https://github.com/huggingface/nanotron/pull/174">PR</a>将模型转换为 Nanotron 格式，解压权重（检查函数<a href="https://gist.github.com/MekkCyber/78c1532e8767e8da0588b778faf61866">unpack_weights</a>），并使用 lighteval。</p>
<p>请注意，尽管这些模型是从一个 Instruct-tuned 模型微调而来，它们仍需要使用 Instruct 数据集进行微调。这些可以被视为基础模型。</p>
<h2 id="_7">使用的算子和测试标准<a class="headerlink" href="#_7" title="Permanent link">&para;</a></h2>
<p>为了从 BitNet 低精度权重中受益，我们将它们打包成一个<code>int8</code> 张量（这使得参数数量从 80 B降至 28 B！）。在推理过程中，这些权重在执行矩阵乘法之前必须进行解包。我们在 Cuda 和 Triton 中实现了自定义内核，以处理矩阵乘法过程中的即时解包。对于矩阵乘法本身，我们采用了缓存分块矩阵乘法技术。为了充分理解这种方法，让我们首先回顾一些 Cuda 编程基础知识。</p>
<h3 id="gpu">基础的GPU概念: 线程, 块, 和共享内存<a class="headerlink" href="#gpu" title="Permanent link">&para;</a></h3>
<p>在深入了解缓存分块矩阵乘法之前，了解一些基本的 GPU 概念是很重要的：</p>
<ul>
<li><strong>线程(thread)和块(block)</strong>：GPU 同时执行成千上万个线程。这些线程被分组成块，每个块独立运行。网格由这些块(grid)组成，代表整个程序空间。例如，在矩阵乘法中，每个线程可能负责计算输出矩阵的一个单元。</li>
<li><strong>共享内存(share memory)</strong>：每个块都可以访问有限量的共享内存，比全局内存（global memory, GPU 上的主内存）要快得多。然而，共享内存大小有限，并在块内的所有线程之间共享。有效利用共享内存是提高 GPU 程序性能的关键。</li>
</ul>
<h3 id="_8">矩阵乘法中的挑战<a class="headerlink" href="#_8" title="Permanent link">&para;</a></h3>
<p>在 GPU 上简单实现矩阵乘法可能涉及每个线程通过直接从全局内存读取所需元素来计算结果矩阵的单个元素。然而，这种方法可能效率低下，原因如下：</p>
<ul>
<li><strong>内存带宽</strong>：相对于 GPU 核心执行计算的速度，访问全局内存相对较慢。如果每个线程直接从全局内存读取矩阵元素，访存时间可能成为瓶颈。</li>
<li><strong>冗余数据访问</strong>：在矩阵乘法中，输入矩阵的许多元素被多次使用。如果每个线程独立从全局内存获取所需数据，相同的数据可能会被多次加载到 GPU 中，导致效率低下。例如，如果每个线程用于计算输出矩阵中的单个元素，则负责计算位置 (i, j) 的线程将需要从全局内存加载矩阵 A 的第 i 行和矩阵 B 的第 j 列。然而，其他线程，例如负责计算位置 (i+1, j) 的线程，无法重用这些数据，将不得不再次从全局内存中加载相同的第 j 列。</li>
</ul>
<h3 id="_9">分块的概念<a class="headerlink" href="#_9" title="Permanent link">&para;</a></h3>
<p>分块是一种用于解决这些挑战的技术，主要用于 FlashAttention 技术中以提高内核的效率。基本思想是将矩阵分成更小的子矩阵，称为块(tile)，这些块可以适应 GPU 的共享内存。计算不再一次完成整个输出矩阵，而是将计算分解为小块，逐块处理。</p>
<p>在矩阵乘法的背景下，这意味着将矩阵 A 和 B 划分为块，将这些块加载到共享内存中，然后在这些较小的块上执行乘法。这种方法允许线程重复使用存储在快速共享内存中的数据，减少了重复访问全局内存的需求。</p>
<p>具体操作如下：</p>
<ul>
<li><strong>将块加载到共享内存</strong>：每个线程块协同地将矩阵 A 的一个小块和相应的矩阵 B 的一个小块从全局内存加载到共享内存。这个操作对每个小块只执行一次，然后该小块被块中的线程多次重复使用。</li>
<li><strong>计算部分乘积</strong>：一旦块加载到共享内存中，每个线程计算部分乘积。由于块中的所有线程都在共享内存中的相同块上工作，它们可以有效地重复使用数据，而无需额外访问全局内存。</li>
<li><strong>累积结果</strong>：计算完一个块的部分乘积后，线程将从矩阵 A 和 B 中加载下一个块到共享内存，并重复这个过程。结果累积在寄存器（或本地内存）中，一旦所有块都被处理，输出矩阵元素的最终值将被写回全局内存。</li>
</ul>
<figure style="text-align: center;">
  <a class="glightbox" href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/illustration_tiling.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/illustration_tiling.png" alt="分块矩阵乘法图示" style="width: 100%;"/></a>
  <figcaption>分块矩阵乘法图示 (来源 https://cnugteren.github.io/tutorial/pages/page4.html)</figcaption>
</figure>

<p><strong>现实的考虑</strong></p>
<p>在实现缓存分块矩阵乘法时，考虑了几个因素：</p>
<ul>
<li><strong>块大小</strong>：块的大小应该选择以平衡能够放入共享内存的数据量和全局内存访问次数之间的权衡。</li>
<li><strong>内存合并</strong>：全局内存访问应该进行内存合并，这意味着相邻的线程访问相邻的内存位置。</li>
<li><strong>占用率</strong>：应该选择每个块中的线程数和网格中的块数，以确保高占用率，即在 GPU 上有尽可能多的活动线程束(warp)（一个线程束是一组 32 个线程），以隐藏内存延迟。</li>
</ul>
<h3 id="triton">Triton算子<a class="headerlink" href="#triton" title="Permanent link">&para;</a></h3>
<p>下面是我们作为基准的一个triton算子:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-9-1"><a id="__codelineno-9-1" name="__codelineno-9-1" href="#__codelineno-9-1"></a><span class="nd">@triton</span><span class="o">.</span><span class="n">autotune</span><span class="p">(</span>
</span><span id="__span-9-2"><a id="__codelineno-9-2" name="__codelineno-9-2" href="#__codelineno-9-2"></a>    <span class="n">configs</span><span class="o">=</span><span class="n">get_cuda_autotune_config</span><span class="p">(),</span>
</span><span id="__span-9-3"><a id="__codelineno-9-3" name="__codelineno-9-3" href="#__codelineno-9-3"></a>    <span class="n">key</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;M&#39;</span><span class="p">,</span> <span class="s1">&#39;N&#39;</span><span class="p">,</span> <span class="s1">&#39;K&#39;</span><span class="p">],</span>
</span><span id="__span-9-4"><a id="__codelineno-9-4" name="__codelineno-9-4" href="#__codelineno-9-4"></a><span class="p">)</span>
</span><span id="__span-9-5"><a id="__codelineno-9-5" name="__codelineno-9-5" href="#__codelineno-9-5"></a><span class="nd">@triton</span><span class="o">.</span><span class="n">jit</span>
</span><span id="__span-9-6"><a id="__codelineno-9-6" name="__codelineno-9-6" href="#__codelineno-9-6"></a><span class="k">def</span> <span class="nf">matmul_kernel</span><span class="p">(</span>
</span><span id="__span-9-7"><a id="__codelineno-9-7" name="__codelineno-9-7" href="#__codelineno-9-7"></a>        <span class="n">a_ptr</span><span class="p">,</span> <span class="n">b_ptr</span><span class="p">,</span> <span class="n">c_ptr</span><span class="p">,</span>
</span><span id="__span-9-8"><a id="__codelineno-9-8" name="__codelineno-9-8" href="#__codelineno-9-8"></a>        <span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span>
</span><span id="__span-9-9"><a id="__codelineno-9-9" name="__codelineno-9-9" href="#__codelineno-9-9"></a>        <span class="n">stride_am</span><span class="p">,</span> <span class="n">stride_ak</span><span class="p">,</span>
</span><span id="__span-9-10"><a id="__codelineno-9-10" name="__codelineno-9-10" href="#__codelineno-9-10"></a>        <span class="n">stride_bk</span><span class="p">,</span> <span class="n">stride_bn</span><span class="p">,</span> 
</span><span id="__span-9-11"><a id="__codelineno-9-11" name="__codelineno-9-11" href="#__codelineno-9-11"></a>        <span class="n">stride_cm</span><span class="p">,</span> <span class="n">stride_cn</span><span class="p">,</span>
</span><span id="__span-9-12"><a id="__codelineno-9-12" name="__codelineno-9-12" href="#__codelineno-9-12"></a>        <span class="n">BLOCK_SIZE_M</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span> <span class="n">BLOCK_SIZE_N</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span> <span class="n">BLOCK_SIZE_K</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>  
</span><span id="__span-9-13"><a id="__codelineno-9-13" name="__codelineno-9-13" href="#__codelineno-9-13"></a>        <span class="n">GROUP_SIZE_M</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>
</span><span id="__span-9-14"><a id="__codelineno-9-14" name="__codelineno-9-14" href="#__codelineno-9-14"></a><span class="p">):</span>
</span><span id="__span-9-15"><a id="__codelineno-9-15" name="__codelineno-9-15" href="#__codelineno-9-15"></a>
</span><span id="__span-9-16"><a id="__codelineno-9-16" name="__codelineno-9-16" href="#__codelineno-9-16"></a>    <span class="n">pid</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">program_id</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span id="__span-9-17"><a id="__codelineno-9-17" name="__codelineno-9-17" href="#__codelineno-9-17"></a>    <span class="n">num_pid_m</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">cdiv</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">BLOCK_SIZE_M</span><span class="p">)</span>
</span><span id="__span-9-18"><a id="__codelineno-9-18" name="__codelineno-9-18" href="#__codelineno-9-18"></a>    <span class="n">num_pid_n</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">cdiv</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">BLOCK_SIZE_N</span><span class="p">)</span>
</span><span id="__span-9-19"><a id="__codelineno-9-19" name="__codelineno-9-19" href="#__codelineno-9-19"></a>    <span class="n">num_pid_in_group</span> <span class="o">=</span> <span class="n">GROUP_SIZE_M</span> <span class="o">*</span> <span class="n">num_pid_n</span>
</span><span id="__span-9-20"><a id="__codelineno-9-20" name="__codelineno-9-20" href="#__codelineno-9-20"></a>    <span class="n">group_id</span> <span class="o">=</span> <span class="n">pid</span> <span class="o">//</span> <span class="n">num_pid_in_group</span>
</span><span id="__span-9-21"><a id="__codelineno-9-21" name="__codelineno-9-21" href="#__codelineno-9-21"></a>    <span class="n">first_pid_m</span> <span class="o">=</span> <span class="n">group_id</span> <span class="o">*</span> <span class="n">GROUP_SIZE_M</span>
</span><span id="__span-9-22"><a id="__codelineno-9-22" name="__codelineno-9-22" href="#__codelineno-9-22"></a>    <span class="n">group_size_m</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">num_pid_m</span> <span class="o">-</span> <span class="n">first_pid_m</span><span class="p">,</span> <span class="n">GROUP_SIZE_M</span><span class="p">)</span>
</span><span id="__span-9-23"><a id="__codelineno-9-23" name="__codelineno-9-23" href="#__codelineno-9-23"></a>    <span class="n">pid_m</span> <span class="o">=</span> <span class="n">first_pid_m</span> <span class="o">+</span> <span class="p">((</span><span class="n">pid</span> <span class="o">%</span> <span class="n">num_pid_in_group</span><span class="p">)</span> <span class="o">%</span> <span class="n">group_size_m</span><span class="p">)</span>
</span><span id="__span-9-24"><a id="__codelineno-9-24" name="__codelineno-9-24" href="#__codelineno-9-24"></a>    <span class="n">pid_n</span> <span class="o">=</span> <span class="p">(</span><span class="n">pid</span> <span class="o">%</span> <span class="n">num_pid_in_group</span><span class="p">)</span> <span class="o">//</span> <span class="n">group_size_m</span>
</span><span id="__span-9-25"><a id="__codelineno-9-25" name="__codelineno-9-25" href="#__codelineno-9-25"></a>
</span><span id="__span-9-26"><a id="__codelineno-9-26" name="__codelineno-9-26" href="#__codelineno-9-26"></a>    <span class="n">offs_am</span> <span class="o">=</span> <span class="p">(</span><span class="n">pid_m</span> <span class="o">*</span> <span class="n">BLOCK_SIZE_M</span> <span class="o">+</span> <span class="n">tl</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_SIZE_M</span><span class="p">))</span> <span class="o">%</span> <span class="n">M</span>
</span><span id="__span-9-27"><a id="__codelineno-9-27" name="__codelineno-9-27" href="#__codelineno-9-27"></a>    <span class="n">offs_bn</span> <span class="o">=</span> <span class="p">(</span><span class="n">pid_n</span> <span class="o">*</span> <span class="n">BLOCK_SIZE_N</span> <span class="o">+</span> <span class="n">tl</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_SIZE_N</span><span class="p">))</span> <span class="o">%</span> <span class="n">N</span>
</span><span id="__span-9-28"><a id="__codelineno-9-28" name="__codelineno-9-28" href="#__codelineno-9-28"></a>    <span class="n">offs_k</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_SIZE_K</span><span class="p">)</span>
</span><span id="__span-9-29"><a id="__codelineno-9-29" name="__codelineno-9-29" href="#__codelineno-9-29"></a>    <span class="n">a_ptrs</span> <span class="o">=</span> <span class="n">a_ptr</span> <span class="o">+</span> <span class="p">(</span><span class="n">offs_am</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_am</span> <span class="o">+</span> <span class="n">offs_k</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_ak</span><span class="p">)</span>
</span><span id="__span-9-30"><a id="__codelineno-9-30" name="__codelineno-9-30" href="#__codelineno-9-30"></a>    <span class="n">b_ptrs</span> <span class="o">=</span> <span class="n">b_ptr</span> <span class="o">+</span> <span class="p">(</span><span class="n">offs_k</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_bk</span> <span class="o">+</span> <span class="n">offs_bn</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_bn</span><span class="p">)</span>
</span><span id="__span-9-31"><a id="__codelineno-9-31" name="__codelineno-9-31" href="#__codelineno-9-31"></a>
</span><span id="__span-9-32"><a id="__codelineno-9-32" name="__codelineno-9-32" href="#__codelineno-9-32"></a>    <span class="n">accumulator</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">BLOCK_SIZE_M</span><span class="p">,</span> <span class="n">BLOCK_SIZE_N</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tl</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
</span><span id="__span-9-33"><a id="__codelineno-9-33" name="__codelineno-9-33" href="#__codelineno-9-33"></a>
</span><span id="__span-9-34"><a id="__codelineno-9-34" name="__codelineno-9-34" href="#__codelineno-9-34"></a>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span> <span class="p">:</span> 
</span><span id="__span-9-35"><a id="__codelineno-9-35" name="__codelineno-9-35" href="#__codelineno-9-35"></a>        <span class="n">b_ptrs</span> <span class="o">=</span> <span class="n">b_ptr</span> <span class="o">+</span> <span class="p">(</span><span class="n">offs_k</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_bk</span> <span class="o">+</span> <span class="n">offs_bn</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_bn</span><span class="p">)</span>
</span><span id="__span-9-36"><a id="__codelineno-9-36" name="__codelineno-9-36" href="#__codelineno-9-36"></a>        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">tl</span><span class="o">.</span><span class="n">cdiv</span><span class="p">(</span><span class="n">K</span> <span class="o">//</span> <span class="mi">4</span><span class="p">,</span> <span class="n">BLOCK_SIZE_K</span><span class="p">)</span> <span class="p">):</span>
</span><span id="__span-9-37"><a id="__codelineno-9-37" name="__codelineno-9-37" href="#__codelineno-9-37"></a>            <span class="n">k</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="n">tl</span><span class="o">.</span><span class="n">cdiv</span><span class="p">(</span><span class="n">K</span> <span class="o">//</span> <span class="mi">4</span><span class="p">,</span> <span class="n">BLOCK_SIZE_K</span><span class="p">)</span> <span class="o">+</span> <span class="n">j</span> 
</span><span id="__span-9-38"><a id="__codelineno-9-38" name="__codelineno-9-38" href="#__codelineno-9-38"></a>
</span><span id="__span-9-39"><a id="__codelineno-9-39" name="__codelineno-9-39" href="#__codelineno-9-39"></a>            <span class="c1"># BLOCK_SIZE_K must be a divisor of K / 4 </span>
</span><span id="__span-9-40"><a id="__codelineno-9-40" name="__codelineno-9-40" href="#__codelineno-9-40"></a>            <span class="n">a</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">a_ptrs</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">offs_k</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">&lt;</span> <span class="n">K</span> <span class="o">-</span> <span class="n">k</span> <span class="o">*</span> <span class="n">BLOCK_SIZE_K</span><span class="p">,</span> <span class="n">other</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span id="__span-9-41"><a id="__codelineno-9-41" name="__codelineno-9-41" href="#__codelineno-9-41"></a>            <span class="n">b_uint8</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">b_ptrs</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">offs_k</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">K</span> <span class="o">//</span> <span class="mi">4</span> <span class="o">-</span> <span class="n">j</span> <span class="o">*</span> <span class="n">BLOCK_SIZE_K</span><span class="p">,</span> <span class="n">other</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span id="__span-9-42"><a id="__codelineno-9-42" name="__codelineno-9-42" href="#__codelineno-9-42"></a>            <span class="n">mask</span> <span class="o">=</span> <span class="mi">3</span><span class="o">&lt;&lt;</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">i</span><span class="p">)</span>
</span><span id="__span-9-43"><a id="__codelineno-9-43" name="__codelineno-9-43" href="#__codelineno-9-43"></a>            <span class="n">b</span> <span class="o">=</span> <span class="p">((</span><span class="n">b_uint8</span> <span class="o">&amp;</span> <span class="n">mask</span><span class="p">)</span> <span class="o">&gt;&gt;</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">i</span><span class="p">))</span>
</span><span id="__span-9-44"><a id="__codelineno-9-44" name="__codelineno-9-44" href="#__codelineno-9-44"></a>
</span><span id="__span-9-45"><a id="__codelineno-9-45" name="__codelineno-9-45" href="#__codelineno-9-45"></a>            <span class="c1"># We accumulate the tiles along the K dimension.</span>
</span><span id="__span-9-46"><a id="__codelineno-9-46" name="__codelineno-9-46" href="#__codelineno-9-46"></a>            <span class="n">tensor_full</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">1</span><span class="p">,),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tl</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
</span><span id="__span-9-47"><a id="__codelineno-9-47" name="__codelineno-9-47" href="#__codelineno-9-47"></a>
</span><span id="__span-9-48"><a id="__codelineno-9-48" name="__codelineno-9-48" href="#__codelineno-9-48"></a>            <span class="n">accumulator</span> <span class="o">+=</span> <span class="n">tl</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">tl</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span> <span class="o">-</span> <span class="n">tensor_full</span><span class="p">),</span> <span class="n">out_dtype</span><span class="o">=</span><span class="n">tl</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
</span><span id="__span-9-49"><a id="__codelineno-9-49" name="__codelineno-9-49" href="#__codelineno-9-49"></a>
</span><span id="__span-9-50"><a id="__codelineno-9-50" name="__codelineno-9-50" href="#__codelineno-9-50"></a>            <span class="n">a_ptrs</span> <span class="o">+=</span> <span class="n">BLOCK_SIZE_K</span> <span class="o">*</span> <span class="n">stride_ak</span>
</span><span id="__span-9-51"><a id="__codelineno-9-51" name="__codelineno-9-51" href="#__codelineno-9-51"></a>            <span class="n">b_ptrs</span> <span class="o">+=</span> <span class="n">BLOCK_SIZE_K</span> <span class="o">*</span> <span class="n">stride_bk</span>
</span><span id="__span-9-52"><a id="__codelineno-9-52" name="__codelineno-9-52" href="#__codelineno-9-52"></a>
</span><span id="__span-9-53"><a id="__codelineno-9-53" name="__codelineno-9-53" href="#__codelineno-9-53"></a>    <span class="n">c</span> <span class="o">=</span> <span class="n">accumulator</span>
</span><span id="__span-9-54"><a id="__codelineno-9-54" name="__codelineno-9-54" href="#__codelineno-9-54"></a>
</span><span id="__span-9-55"><a id="__codelineno-9-55" name="__codelineno-9-55" href="#__codelineno-9-55"></a>    <span class="n">offs_cm</span> <span class="o">=</span> <span class="n">pid_m</span> <span class="o">*</span> <span class="n">BLOCK_SIZE_M</span> <span class="o">+</span> <span class="n">tl</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_SIZE_M</span><span class="p">)</span>
</span><span id="__span-9-56"><a id="__codelineno-9-56" name="__codelineno-9-56" href="#__codelineno-9-56"></a>    <span class="n">offs_cn</span> <span class="o">=</span> <span class="n">pid_n</span> <span class="o">*</span> <span class="n">BLOCK_SIZE_N</span> <span class="o">+</span> <span class="n">tl</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_SIZE_N</span><span class="p">)</span>
</span><span id="__span-9-57"><a id="__codelineno-9-57" name="__codelineno-9-57" href="#__codelineno-9-57"></a>    <span class="n">c_ptrs</span> <span class="o">=</span> <span class="n">c_ptr</span> <span class="o">+</span> <span class="n">stride_cm</span> <span class="o">*</span> <span class="n">offs_cm</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">+</span> <span class="n">stride_cn</span> <span class="o">*</span> <span class="n">offs_cn</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
</span><span id="__span-9-58"><a id="__codelineno-9-58" name="__codelineno-9-58" href="#__codelineno-9-58"></a>    <span class="n">c_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">offs_cm</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">M</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">offs_cn</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">)</span>
</span><span id="__span-9-59"><a id="__codelineno-9-59" name="__codelineno-9-59" href="#__codelineno-9-59"></a>    <span class="n">tl</span><span class="o">.</span><span class="n">store</span><span class="p">(</span><span class="n">c_ptrs</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">c_mask</span><span class="p">)</span>
</span><span id="__span-9-60"><a id="__codelineno-9-60" name="__codelineno-9-60" href="#__codelineno-9-60"></a>
</span><span id="__span-9-61"><a id="__codelineno-9-61" name="__codelineno-9-61" href="#__codelineno-9-61"></a>
</span><span id="__span-9-62"><a id="__codelineno-9-62" name="__codelineno-9-62" href="#__codelineno-9-62"></a><span class="k">def</span> <span class="nf">matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
</span><span id="__span-9-63"><a id="__codelineno-9-63" name="__codelineno-9-63" href="#__codelineno-9-63"></a>    <span class="k">assert</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;Incompatible dimensions, the weight matrix need to be packed&quot;</span>
</span><span id="__span-9-64"><a id="__codelineno-9-64" name="__codelineno-9-64" href="#__codelineno-9-64"></a>    <span class="k">assert</span> <span class="n">a</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">(),</span> <span class="s2">&quot;Matrix A must be contiguous&quot;</span>
</span><span id="__span-9-65"><a id="__codelineno-9-65" name="__codelineno-9-65" href="#__codelineno-9-65"></a>    <span class="n">M</span><span class="p">,</span> <span class="n">K</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span>
</span><span id="__span-9-66"><a id="__codelineno-9-66" name="__codelineno-9-66" href="#__codelineno-9-66"></a>    <span class="n">_</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span>
</span><span id="__span-9-67"><a id="__codelineno-9-67" name="__codelineno-9-67" href="#__codelineno-9-67"></a>    <span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
</span><span id="__span-9-68"><a id="__codelineno-9-68" name="__codelineno-9-68" href="#__codelineno-9-68"></a>    <span class="n">grid</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">META</span><span class="p">:</span> <span class="p">(</span><span class="n">triton</span><span class="o">.</span><span class="n">cdiv</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">META</span><span class="p">[</span><span class="s1">&#39;BLOCK_SIZE_M&#39;</span><span class="p">])</span> <span class="o">*</span> <span class="n">triton</span><span class="o">.</span><span class="n">cdiv</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">META</span><span class="p">[</span><span class="s1">&#39;BLOCK_SIZE_N&#39;</span><span class="p">]),</span> <span class="p">)</span>
</span><span id="__span-9-69"><a id="__codelineno-9-69" name="__codelineno-9-69" href="#__codelineno-9-69"></a>    <span class="n">matmul_kernel</span><span class="p">[</span><span class="n">grid</span><span class="p">](</span>
</span><span id="__span-9-70"><a id="__codelineno-9-70" name="__codelineno-9-70" href="#__codelineno-9-70"></a>        <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span>
</span><span id="__span-9-71"><a id="__codelineno-9-71" name="__codelineno-9-71" href="#__codelineno-9-71"></a>        <span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span>
</span><span id="__span-9-72"><a id="__codelineno-9-72" name="__codelineno-9-72" href="#__codelineno-9-72"></a>        <span class="n">a</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">a</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
</span><span id="__span-9-73"><a id="__codelineno-9-73" name="__codelineno-9-73" href="#__codelineno-9-73"></a>        <span class="n">b</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">b</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
</span><span id="__span-9-74"><a id="__codelineno-9-74" name="__codelineno-9-74" href="#__codelineno-9-74"></a>        <span class="n">c</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">c</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
</span><span id="__span-9-75"><a id="__codelineno-9-75" name="__codelineno-9-75" href="#__codelineno-9-75"></a>    <span class="p">)</span>
</span><span id="__span-9-76"><a id="__codelineno-9-76" name="__codelineno-9-76" href="#__codelineno-9-76"></a>    <span class="k">return</span> <span class="n">c</span>
</span></code></pre></div>
<h3 id="_10">代码解析<a class="headerlink" href="#_10" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>确定分块位置</strong></li>
</ol>
<p>算子首先确定每个线程块负责的输出矩阵的块（tile）：</p>
<ul>
<li><code>pid</code> 是每个线程块的唯一标识符，使用 <code>tl.program_id(axis=0)</code> 获得。</li>
<li>网格被分成一组线程块（<code>GROUP_SIZE_M</code>）。每个组处理输出矩阵的一部分。</li>
<li><code>pid_m</code> 和 <code>pid_n</code> 是分块在 M 和 N 维度上的坐标，分别表示。</li>
<li>
<p>计算偏移量（<code>offs_am</code>、<code>offs_bn</code>、<code>offs_k</code>）以确定每个块中的线程将处理矩阵 A 和 B 的哪些元素。</p>
</li>
<li>
<p><strong>加载和计算分块</strong></p>
</li>
</ul>
<p>算子使用循环以 <code>BLOCK_SIZE_K</code> 的块大小迭代 K 维度。对于每个块：</p>
<ul>
<li><strong>加载分块</strong>：从全局内存加载矩阵 A 和 B 的分块。</li>
<li><strong>解包矩阵 B</strong>：算子假设矩阵 B 是使用 <code>int8</code> 值打包的，这意味着每个元素实际上代表四个较小的值打包成一个字节。解压过程发生在循环内：<ul>
<li>从全局内存加载 <code>b_uint8</code> 作为打包的 <code>int8</code>。</li>
<li>解压每个打包的值以获得用于计算的实际权重值。</li>
</ul>
</li>
<li>
<p><strong>点积</strong>：内核计算从矩阵 A 和 B 加载的分块的点积，并将结果累积到 <code>accumulator</code> 中。<code>accumulator</code> 存储输出矩阵 C 的分块的部分结果。</p>
</li>
<li>
<p><strong>存储结果</strong></p>
</li>
</ul>
<p>在处理完沿着 K 维度的所有分块之后，存储在 <code>accumulator</code> 中的最终结果被转换为 <code>float16</code>，并写回到全局内存中矩阵 C 的相应分块。写入过程使用掩码来确定内存边界，以确保只写入有效元素。</p>
<p>要获取代码的更详细解释，请查看这个<a href="https://github.com/linkedin/Liger-Kernel/pull/195/files">PR</a>。</p>
<h3 id="_11">基准测试<a class="headerlink" href="#_11" title="Permanent link">&para;</a></h3>
<p>我们对我们的算子进行了基准测试，与使用 <code>@torch.compile</code> 解压权重然后在 BF16 精度下执行矩阵乘法的方法进行了对比，发现两种方法的性能几乎相同。为了确保准确的基准测试，我们在 2000 次迭代中执行了矩阵乘法操作，并在最后 1000 次迭代中计算平均时间，以消除与初始加载或编译相关的任何低效性。下面是显示基准测试结果的图表。我们还测试了各种矩阵大小，其中 x 轴表示对数尺度上的乘法次数，y 轴显示平均时间（以毫秒为单位）。</p>
<figure style="text-align: center;">
  <a class="glightbox" href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/without_bitblas.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/without_bitblas.png" alt="Triton算子对比torch.compile" style="width: 100%;"/></a>
  <figcaption>Triton算子对比torch.compile</figcaption>
</figure>

<p>我们还尝试使用 BitBlas，这是一个旨在使用混合精度执行矩阵运算的软件库。它通过允许在较低精度格式（如 INT8、INT4，甚至 INT2）而不是传统的 FP32 或 FP16 格式中进行计算，来帮助优化这些操作。</p>
<p>基准测试结果令人鼓舞，如图所示，BitBlas 在低精度下优于我们的自定义内核和Torch的 <code>matmul</code> 函数。</p>
<figure style="text-align: center;">
  <a class="glightbox" href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/with_bitblas.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/with_bitblas.png" alt="Bitblas 测试" style="width: 100%;"/></a>
  <figcaption>Bitblas测试</figcaption>
</figure>

<p>然而，在模型加载过程中，BitBlas 需要编译适合权重矩阵形状的内核，并将它们存储在本地代码库中，这可能会增加初始加载时间。</p>
<h2 id="_12">结论<a class="headerlink" href="#_12" title="Permanent link">&para;</a></h2>
<p>总之，随着大型语言模型的不断扩展，通过量化来减少它们的计算需求至关重要。本博文探讨了 1.58 位量化的方法，该方法使用了三值权重。虽然在 1.58 位进行预训练模型是资源密集型的，但我们已经证明，通过一些技巧，可以将现有模型微调到这个精度水平，实现高效的性能而不牺牲准确性。通过专门的内核优化推理速度，BitNet 为使大型语言模型更具实用性和可扩展性打开了新的可能性。</p>
<h2 id="_13">致谢<a class="headerlink" href="#_13" title="Permanent link">&para;</a></h2>
<p>我们要衷心感谢 Leandro von Werra、Thomas Wolf 和 Marc Sun 在整个项目中提供的宝贵帮助和见解。我们还要感谢 Omar Sanseviero 和 Pedro Cuenca 在完善这篇博文方面的贡献，帮助我们清晰有效地向人工智能社区传达我们的发现。此外，我们要感谢GeneralAI团队在BitNet项目上的开创性工作。他们的研究对我们的努力具有基础性意义，我们特别感谢他们在论文中提供的清晰准确的数据。</p>
<h2 id="_14">更多资源<a class="headerlink" href="#_14" title="Permanent link">&para;</a></h2>
<ol>
<li>H. Wang et al., <em>BitNet: Scaling 1-bit Transformers for Large Language Models</em>. <a href="https://arxiv.org/pdf/2310.11453">arxiv paper</a></li>
<li>S. Ma et al., <em>The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits</em>. <a href="https://arxiv.org/pdf/2402.17764">arxiv paper</a></li>
<li>S. Ma et al., <em>The Era of 1-bit LLMs: Training Tips, Code and FAQ</em>. <a href="https://github.com/microsoft/unilm/blob/master/bitnet/The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ.pdf">link</a></li>
<li>RJ. Honicky, <em>Are All Large Language Models Really in 1.58 Bits?</em>. <a href="https://learning-exhaust.hashnode.dev/are-all-large-language-models-really-in-158-bits">blogpost</a></li>
<li>L. Mao, <em>CUDA Matrix Multiplication Optimization</em>. <a href="https://leimao.github.io/article/CUDA-Matrix-Multiplication-Optimization/">blogpost</a></li>
<li><em>Tutorial: OpenCL SGEMM tuning for Kepler</em>. <a href="https://cnugteren.github.io/tutorial/pages/page4.html">link</a></li>
<li><em>CUDAMODE</em>. <a href="https://github.com/cuda-mode">github</a>, <a href="https://www.youtube.com/channel/UCJgIbYl6C5no72a0NUAPcTA">youtube</a></li>
<li>Wen-mei W. Hwu, David B. Kirk, Izzat El Hajj, <em>Programming Massively Parallel Processors : A Hands-on Approach</em></li>
</ol>







  
    
  
  
    
  


  <aside class="md-source-file">
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Last update">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">December 1, 2024</span>
  </span>

    
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Created">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">December 1, 2024</span>
  </span>

    
    
      
  
  <span class="md-source-file__fact">
    <span class="md-icon" title="Contributors">
      
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 5.5A3.5 3.5 0 0 1 15.5 9a3.5 3.5 0 0 1-3.5 3.5A3.5 3.5 0 0 1 8.5 9 3.5 3.5 0 0 1 12 5.5M5 8c.56 0 1.08.15 1.53.42-.15 1.43.27 2.85 1.13 3.96C7.16 13.34 6.16 14 5 14a3 3 0 0 1-3-3 3 3 0 0 1 3-3m14 0a3 3 0 0 1 3 3 3 3 0 0 1-3 3c-1.16 0-2.16-.66-2.66-1.62a5.54 5.54 0 0 0 1.13-3.96c.45-.27.97-.42 1.53-.42M5.5 18.25c0-2.07 2.91-3.75 6.5-3.75s6.5 1.68 6.5 3.75V20h-13zM0 20v-1.5c0-1.39 1.89-2.56 4.45-2.9-.59.68-.95 1.62-.95 2.65V20zm24 0h-3.5v-1.75c0-1.03-.36-1.97-.95-2.65 2.56.34 4.45 1.51 4.45 2.9z"/></svg>
      
    </span>
    <nav>
      
    </nav>
  </span>

    
    
  </aside>


  



  <form class="md-feedback" name="feedback" hidden>
    <fieldset>
      <legend class="md-feedback__title">
        Was this page helpful?
      </legend>
      <div class="md-feedback__inner">
        <div class="md-feedback__list">
          
            <button class="md-feedback__icon md-icon" type="submit" title="This page was helpful" data-md-value="1">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 12a8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8 8 8 0 0 0 8-8m2 0a10 10 0 0 1-10 10A10 10 0 0 1 2 12 10 10 0 0 1 12 2a10 10 0 0 1 10 10M10 9.5c0 .8-.7 1.5-1.5 1.5S7 10.3 7 9.5 7.7 8 8.5 8s1.5.7 1.5 1.5m7 0c0 .8-.7 1.5-1.5 1.5S14 10.3 14 9.5 14.7 8 15.5 8s1.5.7 1.5 1.5m-5 7.73c-1.75 0-3.29-.73-4.19-1.81L9.23 14c.45.72 1.52 1.23 2.77 1.23s2.32-.51 2.77-1.23l1.42 1.42c-.9 1.08-2.44 1.81-4.19 1.81"/></svg>
            </button>
          
            <button class="md-feedback__icon md-icon" type="submit" title="This page could be improved" data-md-value="0">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 12a8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8 8 8 0 0 0 8-8m2 0a10 10 0 0 1-10 10A10 10 0 0 1 2 12 10 10 0 0 1 12 2a10 10 0 0 1 10 10m-6.5-4c.8 0 1.5.7 1.5 1.5s-.7 1.5-1.5 1.5-1.5-.7-1.5-1.5.7-1.5 1.5-1.5M10 9.5c0 .8-.7 1.5-1.5 1.5S7 10.3 7 9.5 7.7 8 8.5 8s1.5.7 1.5 1.5m2 4.5c1.75 0 3.29.72 4.19 1.81l-1.42 1.42C14.32 16.5 13.25 16 12 16s-2.32.5-2.77 1.23l-1.42-1.42C8.71 14.72 10.25 14 12 14"/></svg>
            </button>
          
        </div>
        <div class="md-feedback__note">
          
            <div data-md-value="1" hidden>
              
              
                
              
              Thanks for your feedback!
            </div>
          
            <div data-md-value="0" hidden>
              
              
                
              
              Thanks for your feedback! Help us improve this page by using our <a href="..." target="_blank" rel="noopener">feedback form</a>.
            </div>
          
        </div>
      </div>
    </fieldset>
  </form>


                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../../big-data/" class="md-footer__link md-footer__link--prev" aria-label="Previous: 大数据">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256l137.3-137.4c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                大数据
              </div>
            </div>
          </a>
        
        
          
          <a href="../2023-in-llms/" class="md-footer__link md-footer__link--next" aria-label="Next: 2023, 开源大模型之年">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                2023, 开源大模型之年
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2020 - 2024 FastX-AI
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
      
    
    
    
    <a href="https://fastx-ai.com" target="_blank" rel="noopener me" title="fastx-ai" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M433 179.11c0-97.2-63.71-125.7-63.71-125.7-62.52-28.7-228.56-28.4-290.48 0 0 0-63.72 28.5-63.72 125.7 0 115.7-6.6 259.4 105.63 289.1 40.51 10.7 75.32 13 103.33 11.4 50.81-2.8 79.32-18.1 79.32-18.1l-1.7-36.9s-36.31 11.4-77.12 10.1c-40.41-1.4-83-4.4-89.63-54a102.5 102.5 0 0 1-.9-13.9c85.63 20.9 158.65 9.1 178.75 6.7 56.12-6.7 105-41.3 111.23-72.9 9.8-49.8 9-121.5 9-121.5m-75.12 125.2h-46.63v-114.2c0-49.7-64-51.6-64 6.9v62.5h-46.33V197c0-58.5-64-56.6-64-6.9v114.2H90.19c0-122.1-5.2-147.9 18.41-175 25.9-28.9 79.82-30.8 103.83 6.1l11.6 19.5 11.6-19.5c24.11-37.1 78.12-34.8 103.83-6.1 23.71 27.3 18.4 53 18.4 175z"/></svg>
    </a>
  
    
    
    
    
    <a href="mailto:x.stark.dylan@gmail.com" target="_blank" rel="noopener" title="send me an email" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M64 112c-8.8 0-16 7.2-16 16v22.1l172.5 141.6c20.7 17 50.4 17 71.1 0L464 150.1V128c0-8.8-7.2-16-16-16zM48 212.2V384c0 8.8 7.2 16 16 16h384c8.8 0 16-7.2 16-16V212.2L322 328.8c-38.4 31.5-93.7 31.5-132 0zM0 128c0-35.3 28.7-64 64-64h384c35.3 0 64 28.7 64 64v256c0 35.3-28.7 64-64 64H64c-35.3 0-64-28.7-64-64z"/></svg>
    </a>
  
    
    
    
    
    <a href="/contact" target="_blank" rel="noopener" title="contact us" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 4H7a5 5 0 0 0-5 5v11h18a2 2 0 0 0 2-2V9a5 5 0 0 0-5-5m-7 14H4V9a3 3 0 0 1 3-3 3 3 0 0 1 3 3zm9-3h-2v-2h-4v-2h6zM9 11H5V9h4z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
      <div class="md-progress" data-md-component="progress" role="progressbar"></div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.indexes", "navigation.instant.progress", "navigation.tracking", "navigation.tabs", "navigation.top", "navigation.footer", "navigation.prune", "content.action.edit", "content.code.copy", "content.code.annotate", "content.tabs.link", "content.tooltips", "header.autohide", "announce.dismiss", "search.suggest", "search.highlight", "search.share", "toc.follow", "toc.integrate"], "search": "../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
<!-- Add scripts that need to run before here -->

      <script src="../../assets/javascripts/bundle.83f73b43.min.js"></script>
      
        <script src="../../javascripts/extra.js"></script>
      
        <script src="../../javascripts/mathjax.js"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
<!-- Add scripts that need to run afterwards here -->

  <script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(() => { lightbox.reload() });
</script></body>
</html>